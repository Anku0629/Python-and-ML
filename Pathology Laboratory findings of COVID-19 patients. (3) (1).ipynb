{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6497da1a",
   "metadata": {},
   "source": [
    "### Pathology Laboratory findings of COVID-19 patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a4e43",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "This project takes the datasets obtained from Kaggle.\n",
    "\n",
    "Kaggle is a large, freely-available database comprising deidentified health-related data associated with over 40,000 patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2020 and 2021.\n",
    "This repository contains the code to derive the Hematology Complete Blood Count (CBC) as well as Biochemistry tests Dataset from the Kaggle dataset.\n",
    "\n",
    "Derive the Hematology Complete Blood Count (CBC) as well as Biochemestry Tests Dataset from the Kaggle dataset. (Done)\n",
    "\n",
    "This is a classification data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d46a8",
   "metadata": {},
   "source": [
    "### Import all the liabraries -\n",
    "\n",
    "Pandas - Used to analyse data. It has function for analysing,cleaning,exploring and manipulating data.\n",
    "\n",
    "Numpy - Mostly work on numerical values for making Arithmatic Operations.\n",
    "\n",
    "Matplotlib - Comprehensive library for creating static,animated and intractive visualization.\n",
    "\n",
    "Seaborn - Seaborn is a python data visualization library based on matplotlib. It provides a high-level interface for drawing intractive and informative statastical graphics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Warnings - warnings are provided to warn the developer of situation that are not necessarily exceptions and ignore them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6253aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb9a63",
   "metadata": {},
   "source": [
    "#### By read_csv() function we are reading the dataset present in \"Pathology Laboratory findings of COVID-19 patients..csv\" file.\n",
    "\n",
    "Took dataset from Kaggle\n",
    "\n",
    "Will be making predictions models using Machine Learning Algorithams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb5bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"covid19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07aa5a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CK</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00345_2020-03-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.150</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00791_2020-03-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.970</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00741_2020-03-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00605_2020-04-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.755</td>\n",
       "      <td>123.5</td>\n",
       "      <td>176.5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00417_2020-02-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.810</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960</td>\n",
       "      <td>79.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>48.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1736 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0  Sex   Age    CA     CK   CREA    ALP    GGT    GLU  \\\n",
       "0     A00345_2020-03-25  1.0  82.0  2.09    NaN  1.150   95.0   40.0   78.0   \n",
       "1     A00791_2020-03-19  1.0  51.0  1.97  237.0  0.970   54.0   98.0   98.0   \n",
       "2     A00741_2020-03-04  1.0  58.0  2.11    NaN  1.000   80.0  147.0  106.0   \n",
       "3     A00605_2020-04-15  0.0  82.0  2.27  138.0  0.755  123.5  176.5  106.0   \n",
       "4     A00417_2020-02-24  1.0  79.0  2.07   73.0  1.810   62.0   36.5   96.0   \n",
       "...                 ...  ...   ...   ...    ...    ...    ...    ...    ...   \n",
       "1731                 49  0.0   NaN  2.38   40.0  0.800   68.0    9.0  128.0   \n",
       "1732                 50  0.0   NaN  2.36    NaN  0.960   79.0   35.0  107.0   \n",
       "1733                 51  1.0   NaN  2.28    NaN  1.420    NaN    NaN  136.0   \n",
       "1734                 52  1.0   NaN  2.40  124.0  0.950   48.0   44.0   95.0   \n",
       "1735                 53  1.0   NaN  2.14    NaN  1.040    NaN    NaN  179.0   \n",
       "\n",
       "        AST  ...    MO   EO   BA   NET   LYT  MOT  EOT  BAT  Suspect  target  \n",
       "0      26.0  ...   9.5  2.9  0.5  6.40  1.20  0.8  0.3  0.0      1.0       0  \n",
       "1      74.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       1  \n",
       "2      41.0  ...   7.3  0.3  0.1  5.45  0.75  0.5  0.0  0.0      1.0       0  \n",
       "3     114.0  ...   9.5  1.7  0.9  3.60  2.60  0.7  0.1  0.1      0.5       0  \n",
       "4      28.0  ...  10.0  8.5  0.5  0.40  0.50  0.1  0.1  0.0      1.0       0  \n",
       "...     ...  ...   ...  ...  ...   ...   ...  ...  ...  ...      ...     ...  \n",
       "1731   13.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1732   24.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1733   53.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1734   50.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1735   28.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "\n",
       "[1736 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e704f28",
   "metadata": {},
   "source": [
    "### Performing Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14920afb",
   "metadata": {},
   "source": [
    "#### Find information about Data by using df.info()\n",
    "\n",
    "This Covid19.csv dataset contains the information of patients health condition through their tests peroforms during covid19 with 1736 rows and 36 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ac7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1736 entries, 0 to 1735\n",
      "Data columns (total 36 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  1736 non-null   object \n",
      " 1   Sex         1736 non-null   float64\n",
      " 2   Age         1682 non-null   float64\n",
      " 3   CA          1643 non-null   float64\n",
      " 4   CK          704 non-null    float64\n",
      " 5   CREA        1662 non-null   float64\n",
      " 6   ALP         1262 non-null   float64\n",
      " 7   GGT         1300 non-null   float64\n",
      " 8   GLU         1638 non-null   float64\n",
      " 9   AST         1638 non-null   float64\n",
      " 10  ALT         1640 non-null   float64\n",
      " 11  LDH         1433 non-null   float64\n",
      " 12  PCR         1639 non-null   float64\n",
      " 13  KAL         1656 non-null   float64\n",
      " 14  NAT         1663 non-null   float64\n",
      " 15  UREA        1060 non-null   float64\n",
      " 16  WBC         1673 non-null   float64\n",
      " 17  RBC         1673 non-null   float64\n",
      " 18  HGB         1673 non-null   float64\n",
      " 19  HCT         1673 non-null   float64\n",
      " 20  MCV         1673 non-null   float64\n",
      " 21  MCH         1673 non-null   float64\n",
      " 22  MCHC        1673 non-null   float64\n",
      " 23  PLT1        1673 non-null   float64\n",
      " 24  NE          1374 non-null   float64\n",
      " 25  LY          1374 non-null   float64\n",
      " 26  MO          1374 non-null   float64\n",
      " 27  EO          1374 non-null   float64\n",
      " 28  BA          1374 non-null   float64\n",
      " 29  NET         1374 non-null   float64\n",
      " 30  LYT         1374 non-null   float64\n",
      " 31  MOT         1374 non-null   float64\n",
      " 32  EOT         1374 non-null   float64\n",
      " 33  BAT         1374 non-null   float64\n",
      " 34  Suspect     1736 non-null   float64\n",
      " 35  target      1736 non-null   int64  \n",
      "dtypes: float64(34), int64(1), object(1)\n",
      "memory usage: 488.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972623c6",
   "metadata": {},
   "source": [
    "This dataset contain 1736 rows and 36 columns out of there are 35 numerical columns and 1 object columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ac236",
   "metadata": {},
   "source": [
    "#### To find how much null values in DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52785239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "Sex              0\n",
       "Age             54\n",
       "CA              93\n",
       "CK            1032\n",
       "CREA            74\n",
       "ALP            474\n",
       "GGT            436\n",
       "GLU             98\n",
       "AST             98\n",
       "ALT             96\n",
       "LDH            303\n",
       "PCR             97\n",
       "KAL             80\n",
       "NAT             73\n",
       "UREA           676\n",
       "WBC             63\n",
       "RBC             63\n",
       "HGB             63\n",
       "HCT             63\n",
       "MCV             63\n",
       "MCH             63\n",
       "MCHC            63\n",
       "PLT1            63\n",
       "NE             362\n",
       "LY             362\n",
       "MO             362\n",
       "EO             362\n",
       "BA             362\n",
       "NET            362\n",
       "LYT            362\n",
       "MOT            362\n",
       "EOT            362\n",
       "BAT            362\n",
       "Suspect          0\n",
       "target           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c261bb",
   "metadata": {},
   "source": [
    "With the help of isnull().sum() function we got the null count of all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787e176f",
   "metadata": {},
   "source": [
    "#### Drop unwanted columns by using drop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f8c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"CK\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e05b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a5ebf",
   "metadata": {},
   "source": [
    "As the missing Data is more than 50% so we can drop the column \"CK\" and \"Unnamed: 0\" is unwanted column so we can drop it also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea38c9d",
   "metadata": {},
   "source": [
    "#### Drop Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ea894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906bc060",
   "metadata": {},
   "source": [
    "Remaining columns consists null values are 0.5 % ,0.8 % so we can drop that row content null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d850ab21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex        0\n",
       "Age        0\n",
       "CA         0\n",
       "CREA       0\n",
       "ALP        0\n",
       "GGT        0\n",
       "GLU        0\n",
       "AST        0\n",
       "ALT        0\n",
       "LDH        0\n",
       "PCR        0\n",
       "KAL        0\n",
       "NAT        0\n",
       "UREA       0\n",
       "WBC        0\n",
       "RBC        0\n",
       "HGB        0\n",
       "HCT        0\n",
       "MCV        0\n",
       "MCH        0\n",
       "MCHC       0\n",
       "PLT1       0\n",
       "NE         0\n",
       "LY         0\n",
       "MO         0\n",
       "EO         0\n",
       "BA         0\n",
       "NET        0\n",
       "LYT        0\n",
       "MOT        0\n",
       "EOT        0\n",
       "BAT        0\n",
       "Suspect    0\n",
       "target     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6972983",
   "metadata": {},
   "source": [
    " Now, our data does not contains any null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293f1c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>123.5</td>\n",
       "      <td>176.5</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.50</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>264.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>364.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.961070</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>185.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.44</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>3.986000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.923645</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>44.0</td>\n",
       "      <td>206.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>422.3</td>\n",
       "      <td>...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>11.664000</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.411833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.946667</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.012633</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.333333</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>242.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.886667</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.129067</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>81.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>190.666667</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>303.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>3.976667</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age        CA      CREA    ALP    GGT         GLU         AST  \\\n",
       "0     1.0  82.0  2.090000  1.150000   95.0   40.0   78.000000   26.000000   \n",
       "3     0.0  82.0  2.270000  0.755000  123.5  176.5  106.000000  114.000000   \n",
       "4     1.0  79.0  2.070000  1.810000   62.0   36.5   96.000000   28.000000   \n",
       "5     1.0  84.0  2.060000  1.283333   75.0   75.0   95.500000   40.500000   \n",
       "8     0.0  48.0  2.110000  0.660000  200.0   90.0  104.000000   38.000000   \n",
       "...   ...   ...       ...       ...    ...    ...         ...         ...   \n",
       "1644  1.0  68.0  1.961070  0.688000   58.0   21.0   77.800000   12.200000   \n",
       "1650  0.0  79.0  1.923645  0.509000   44.0  206.6   93.000000   46.300000   \n",
       "1652  0.0  76.0  2.411833  0.750000   51.0   32.0   90.000000   25.333333   \n",
       "1678  1.0  58.0  2.012633  0.736667   42.0   20.0  100.333333   21.000000   \n",
       "1681  0.0  78.0  2.129067  0.483333   81.0   33.0  190.666667   21.000000   \n",
       "\n",
       "            ALT    LDH  ...     MO        EO        BA        NET       LYT  \\\n",
       "0     21.000000  307.0  ...   9.50  2.900000  0.500000   6.400000  1.200000   \n",
       "3     63.000000  281.0  ...   9.50  1.700000  0.900000   3.600000  2.600000   \n",
       "4     38.500000  264.0  ...  10.00  8.500000  0.500000   0.400000  0.500000   \n",
       "5     27.000000  364.5  ...   6.80  1.100000  0.600000   2.800000  0.600000   \n",
       "8     36.000000  189.0  ...   9.00  0.500000  0.000000   4.400000  0.700000   \n",
       "...         ...    ...  ...    ...       ...       ...        ...       ...   \n",
       "1644  16.400000  185.0  ...   7.44  1.320000  0.380000   3.986000  1.050000   \n",
       "1650  33.700000  422.3  ...   4.30  2.100000  0.170000  11.664000  0.895000   \n",
       "1652  58.000000  233.0  ...   8.00  2.200000  0.766667   2.946667  1.363333   \n",
       "1678  35.000000  242.5  ...  11.80  1.366667  0.100000   1.886667  1.116667   \n",
       "1681  34.666667  303.0  ...  10.00  3.866667  0.733333   3.976667  1.560000   \n",
       "\n",
       "           MOT       EOT       BAT  Suspect  target  \n",
       "0     0.800000  0.300000  0.000000      1.0       0  \n",
       "3     0.700000  0.100000  0.100000      0.5       0  \n",
       "4     0.100000  0.100000  0.000000      1.0       0  \n",
       "5     0.250000  0.050000  0.000000      0.0       1  \n",
       "8     0.500000  0.000000  0.000000      1.0       0  \n",
       "...        ...       ...       ...      ...     ...  \n",
       "1644  0.404000  0.072000  0.022000      1.0       1  \n",
       "1650  0.577000  0.286000  0.025000      1.0       1  \n",
       "1652  0.386667  0.106667  0.036667      1.0       1  \n",
       "1678  0.410000  0.050000  0.003333      1.0       1  \n",
       "1681  0.646667  0.246667  0.046667      1.0       1  \n",
       "\n",
       "[826 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae1056",
   "metadata": {},
   "source": [
    "#### How many people get infected through covid and how many are not infected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e3048b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    463\n",
       "0    363\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26759eae",
   "metadata": {},
   "source": [
    "Here is 463 are infected peoples and 363 are not infected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e9030",
   "metadata": {},
   "source": [
    "#### What is lowest Age of patient whoes admitted in hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c987fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Age\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91464d2f",
   "metadata": {},
   "source": [
    "The smallest person was admitted in hospital during covid is 17 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01637c",
   "metadata": {},
   "source": [
    "#### whoes age is less than 99 and its having target=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84787082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>123.5</td>\n",
       "      <td>176.5</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.50</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.5</td>\n",
       "      <td>264.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>62.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>...</td>\n",
       "      <td>10.65</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.240000</td>\n",
       "      <td>38.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>169.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>85.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.40</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>4.90</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.48</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>56.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.55</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>319.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age    CA      CREA    ALP    GGT         GLU    AST   ALT    LDH  \\\n",
       "0     1.0  82.0  2.09  1.150000   95.0   40.0   78.000000   26.0  21.0  307.0   \n",
       "3     0.0  82.0  2.27  0.755000  123.5  176.5  106.000000  114.0  63.0  281.0   \n",
       "4     1.0  79.0  2.07  1.810000   62.0   36.5   96.000000   28.0  38.5  264.0   \n",
       "8     0.0  48.0  2.11  0.660000  200.0   90.0  104.000000   38.0  36.0  189.0   \n",
       "12    0.0  53.0  2.36  0.593333   62.0   21.5   91.666667   36.0  35.5  269.5   \n",
       "...   ...   ...   ...       ...    ...    ...         ...    ...   ...    ...   \n",
       "1602  1.0  56.0  2.22  1.240000   38.5   12.0  102.000000   16.5  15.5  169.0   \n",
       "1606  0.0  52.0  2.35  0.520000   85.0   42.0  140.000000   27.0  34.0  196.0   \n",
       "1608  0.0  39.0  2.21  0.830000   26.0   11.0   79.000000   20.0  17.0  156.0   \n",
       "1610  1.0  74.0  2.48  0.940000   56.5   35.0  113.500000   34.0  40.0  335.0   \n",
       "1619  0.0  80.0  2.32  0.680000   95.0   21.5  113.000000   22.0  15.5  319.0   \n",
       "\n",
       "      ...     MO   EO    BA   NET   LYT   MOT   EOT  BAT  Suspect  target  \n",
       "0     ...   9.50  2.9  0.50  6.40  1.20  0.80  0.30  0.0      1.0       0  \n",
       "3     ...   9.50  1.7  0.90  3.60  2.60  0.70  0.10  0.1      0.5       0  \n",
       "4     ...  10.00  8.5  0.50  0.40  0.50  0.10  0.10  0.0      1.0       0  \n",
       "8     ...   9.00  0.5  0.00  4.40  0.70  0.50  0.00  0.0      1.0       0  \n",
       "12    ...  10.65  0.9  0.40  3.95  1.40  0.60  0.05  0.0      1.0       0  \n",
       "...   ...    ...  ...   ...   ...   ...   ...   ...  ...      ...     ...  \n",
       "1602  ...   6.40  0.2  0.45  5.15  1.10  0.45  0.00  0.0      0.0       0  \n",
       "1606  ...  12.40  3.2  0.70  4.90  2.60  1.10  0.30  0.1      0.0       0  \n",
       "1608  ...   5.60  2.5  0.40  4.70  2.30  0.40  0.20  0.0      1.0       0  \n",
       "1610  ...  10.55  3.0  0.90  6.00  2.45  1.05  0.30  0.1      0.0       0  \n",
       "1619  ...   3.50  0.0  0.00  3.60  0.60  0.20  0.00  0.0      0.0       0  \n",
       "\n",
       "[362 rows x 34 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"Age\"]<99)&(df[\"target\"]==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fed27e",
   "metadata": {},
   "source": [
    "There is 362 peoples are not infected according to age was less than 99 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3c100",
   "metadata": {},
   "source": [
    "#### Whoes age is greater than 20 and having glucose level greater than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c077c793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>159.500000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>559.500000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>563.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.230000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>5.343333</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>282.666667</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>24.333333</td>\n",
       "      <td>435.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>6.733333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2.175000</td>\n",
       "      <td>1.395000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>0.923333</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>203.333333</td>\n",
       "      <td>65.333333</td>\n",
       "      <td>509.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>982.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>562.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.766000</td>\n",
       "      <td>65.666667</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>41.333333</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>391.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>314.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>258.500000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.085000</td>\n",
       "      <td>2.385000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.690000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>2.590000</td>\n",
       "      <td>119.500000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>298.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>307.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.930000</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>258.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.075000</td>\n",
       "      <td>4.820000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>2.405000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>240.500000</td>\n",
       "      <td>356.500000</td>\n",
       "      <td>164.500000</td>\n",
       "      <td>991.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>481.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>596.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.930000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>362.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>635.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>513.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.985000</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>212.333333</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.333333</td>\n",
       "      <td>408.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.833333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>1.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.860000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>273.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>54.500000</td>\n",
       "      <td>572.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>341.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>8.450000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>243.500000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.793333</td>\n",
       "      <td>3.883333</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>315.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.985000</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>212.333333</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.333333</td>\n",
       "      <td>408.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.300</td>\n",
       "      <td>9.833333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>3.292500</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>269.500000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>357.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.525000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>548.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.095000</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>279.500000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>447.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.165000</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.075000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>128.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>288.500000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>1.415000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>296.500000</td>\n",
       "      <td>126.500000</td>\n",
       "      <td>1236.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.950000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250</td>\n",
       "      <td>5.150000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>257.500000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.335000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>272.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.550</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.065000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>255.500000</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>311.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.535000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>855.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.195000</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>210.500000</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age        CA      CREA         ALP         GGT         GLU  \\\n",
       "13    0.0  76.0  2.200000  2.420000  159.500000  147.000000  640.000000   \n",
       "31    1.0  78.0  1.700000  1.230000   70.000000   13.000000  278.000000   \n",
       "84    1.0  82.0  2.480000  1.360000   66.000000   21.000000  296.000000   \n",
       "95    1.0  84.0  2.370000  2.350000   73.000000   22.000000  265.000000   \n",
       "107   0.0  86.0  2.230000  2.070000   88.000000   52.000000  267.000000   \n",
       "111   0.0  66.0  2.210000  5.343333   51.000000   19.000000  282.666667   \n",
       "124   0.0  87.0  2.175000  1.395000   81.000000   26.000000  201.000000   \n",
       "125   1.0  66.0  2.370000  1.230000   66.000000   34.000000  450.000000   \n",
       "131   0.0  72.0  2.255000  1.600000   86.000000  146.000000  276.000000   \n",
       "150   1.0  76.0  2.010000  0.923333   62.000000   25.000000  289.000000   \n",
       "241   1.0  67.0  1.940000  1.120000  106.500000   84.000000  206.000000   \n",
       "257   1.0  63.0  2.030000  0.735000   85.000000   54.000000  295.000000   \n",
       "269   0.0  85.0  2.110000  1.190000   78.000000   22.000000  237.000000   \n",
       "277   0.0  73.0  2.090000  1.766000   65.666667   28.333333  260.000000   \n",
       "331   0.0  31.0  2.150000  0.590000   83.000000   75.000000  208.000000   \n",
       "402   1.0  46.0  2.090000  1.080000   49.000000   91.000000  258.500000   \n",
       "410   1.0  62.0  2.085000  2.385000  114.000000   64.000000  274.000000   \n",
       "411   1.0  75.0  2.090000  1.690000   56.000000   60.000000  459.000000   \n",
       "437   1.0  73.0  2.115000  2.590000  119.500000   17.000000  298.500000   \n",
       "441   1.0  44.0  2.300000  1.140000   45.000000  121.000000  207.000000   \n",
       "444   0.0  89.0  1.930000  0.885000  200.000000  377.000000  258.000000   \n",
       "471   0.0  90.0  2.075000  4.820000   96.000000   17.000000  309.000000   \n",
       "500   0.0  88.0  1.363333  2.405000   52.000000   39.000000  240.500000   \n",
       "524   0.0  69.0  2.290000  0.770000   82.000000   44.000000  282.500000   \n",
       "572   1.0  73.0  2.060000  1.470000   67.000000   50.500000  246.000000   \n",
       "700   0.0  43.0  1.930000  0.920000   57.000000  194.000000  450.000000   \n",
       "740   0.0  77.0  2.220000  1.600000  175.000000   17.000000  362.000000   \n",
       "799   0.0  41.0  2.060000  5.720000  201.000000   14.000000  271.000000   \n",
       "862   1.0  56.0  2.060000  1.105000   68.000000   56.000000  257.000000   \n",
       "874   1.0  55.0  1.850000  0.860000  102.000000  128.000000  232.000000   \n",
       "880   0.0  74.0  2.105000  0.660000   63.500000   49.000000  207.000000   \n",
       "921   0.0  82.0  1.985000  2.770000  212.333333  157.666667  386.000000   \n",
       "930   1.0  92.0  2.100000  1.400000   33.000000   35.000000  201.000000   \n",
       "1025  0.0  48.0  1.860000  0.555000  273.000000   58.000000  287.000000   \n",
       "1070  0.0  52.0  2.100000  0.600000  101.000000   28.000000  205.000000   \n",
       "1073  1.0  79.0  2.255000  1.440000   63.000000   36.000000  223.000000   \n",
       "1081  0.0  69.0  2.290000  0.585000   66.000000   30.000000  243.500000   \n",
       "1135  1.0  61.0  2.793333  3.883333   96.000000   22.500000  204.000000   \n",
       "1153  1.0  76.0  1.985000  2.770000  212.333333  157.666667  386.000000   \n",
       "1169  0.0  70.0  2.160000  3.292500   72.500000   21.000000  269.500000   \n",
       "1215  1.0  81.0  2.190000  1.760000   45.000000   22.000000  311.000000   \n",
       "1221  0.0  72.0  2.095000  1.095000   60.000000   12.000000  279.500000   \n",
       "1225  0.0  86.0  2.020000  0.810000  176.000000  237.000000  204.000000   \n",
       "1258  1.0  85.0  2.165000  3.020000   64.000000   60.000000  206.000000   \n",
       "1272  1.0  75.0  2.075000  1.405000  128.500000  200.000000  288.500000   \n",
       "1291  1.0  71.0  1.980000  1.415000   68.000000  262.000000  219.000000   \n",
       "1345  1.0  74.0  1.950000  2.080000   92.000000   91.500000  226.000000   \n",
       "1373  1.0  68.0  2.150000  1.320000   60.000000  133.000000  270.000000   \n",
       "1402  1.0  55.0  2.190000  0.940000  129.000000   99.000000  216.000000   \n",
       "1404  1.0  75.0  2.240000  0.980000  579.000000   79.500000  383.000000   \n",
       "1517  1.0  76.0  2.060000  1.140000   78.500000   52.000000  257.500000   \n",
       "1570  0.0  37.0  2.335000  0.515000   77.000000   35.000000  270.000000   \n",
       "1581  1.0  62.0  2.065000  1.950000   60.500000   27.000000  255.500000   \n",
       "1585  1.0  86.0  2.200000  1.535000  108.000000   13.500000  855.000000   \n",
       "1586  1.0  61.0  2.195000  1.145000   96.000000   14.000000  210.500000   \n",
       "\n",
       "              AST         ALT          LDH  ...         MO         EO     BA  \\\n",
       "13    1018.500000  559.500000   694.000000  ...   7.250000   0.050000  0.050   \n",
       "31      17.000000   10.000000   352.000000  ...   4.100000   0.100000  0.200   \n",
       "84      42.000000   23.000000   300.000000  ...  10.400000   0.200000  0.200   \n",
       "95      83.000000   52.000000   563.000000  ...  16.000000   0.000000  0.000   \n",
       "107    244.000000  133.000000   448.000000  ...   8.300000   0.000000  0.200   \n",
       "111     65.666667   24.333333   435.666667  ...   2.833333   0.000000  0.200   \n",
       "124     45.500000   34.500000   264.000000  ...   6.300000   0.650000  0.500   \n",
       "125     28.000000   27.000000   406.000000  ...   5.600000   0.100000  0.300   \n",
       "131     66.000000   59.000000   313.000000  ...  14.400000   0.800000  1.000   \n",
       "150    203.333333   65.333333   509.666667  ...   5.200000   1.200000  0.300   \n",
       "241    281.000000  153.000000   982.000000  ...   2.250000   0.250000  0.800   \n",
       "257     87.000000   63.000000   562.500000  ...   6.300000   0.100000  0.150   \n",
       "269     53.000000   36.000000   434.000000  ...   9.600000   0.000000  0.200   \n",
       "277     41.333333   18.000000   391.200000  ...   6.800000   0.250000  0.250   \n",
       "331    101.000000  103.000000   314.000000  ...  14.600000   2.800000  0.500   \n",
       "402     39.500000   40.500000   238.000000  ...   7.000000   0.000000  0.100   \n",
       "410     38.000000   40.500000   233.000000  ...   5.000000   0.000000  0.100   \n",
       "411     71.000000   59.000000   430.000000  ...   7.900000   0.000000  0.200   \n",
       "437     18.000000   14.500000   307.500000  ...   7.100000   0.000000  0.450   \n",
       "441     57.000000   52.000000   403.000000  ...   7.000000   0.000000  0.300   \n",
       "444    105.000000   97.500000   742.000000  ...   3.450000   0.000000  0.100   \n",
       "471     27.000000   21.000000   310.000000  ...   5.600000   0.000000  0.200   \n",
       "500    356.500000  164.500000   991.000000  ...   4.200000   0.050000  0.200   \n",
       "524     32.000000   21.000000   481.500000  ...   3.500000   0.750000  0.250   \n",
       "572     75.500000   96.000000   596.500000  ...   5.600000   0.000000  0.200   \n",
       "700     50.000000   38.000000   495.000000  ...   8.000000   0.000000  0.000   \n",
       "740     18.000000   17.000000   332.000000  ...   7.100000   1.000000  0.300   \n",
       "799     16.000000   11.000000   266.000000  ...  13.600000  11.100000  0.700   \n",
       "862     57.000000   46.000000   568.000000  ...   2.500000   0.000000  0.100   \n",
       "874    112.000000   77.000000   635.000000  ...   9.900000   0.000000  0.100   \n",
       "880     34.500000   42.500000   513.500000  ...   2.700000   0.000000  0.900   \n",
       "921     32.000000   32.333333   408.333333  ...   4.900000   0.133333  0.300   \n",
       "930     53.000000   36.000000   393.000000  ...   1.300000   0.000000  0.100   \n",
       "1025    48.500000   54.500000   572.500000  ...  10.400000   0.200000  0.200   \n",
       "1070    64.000000   86.000000   218.000000  ...   7.400000   3.100000  0.200   \n",
       "1073    31.500000   19.500000   341.000000  ...   9.250000   0.000000  0.150   \n",
       "1081    20.500000   27.000000   220.500000  ...   2.500000   0.000000  0.100   \n",
       "1135    21.000000   18.000000   315.333333  ...   5.000000   0.050000  0.200   \n",
       "1153    32.000000   32.333333   408.333333  ...   4.900000   0.133333  0.300   \n",
       "1169    53.500000   58.500000   357.500000  ...   6.525000   0.050000  0.125   \n",
       "1215    90.000000   48.000000   548.000000  ...   1.900000   0.000000  0.000   \n",
       "1221    58.000000   58.500000   381.000000  ...   3.250000   0.000000  0.250   \n",
       "1225    48.000000   52.000000   447.000000  ...   8.600000   0.200000  0.400   \n",
       "1258    13.000000   11.000000   209.000000  ...  17.200000   1.500000  0.100   \n",
       "1272   104.000000   46.500000   398.000000  ...   6.650000   0.000000  0.200   \n",
       "1291    72.500000   51.000000   501.000000  ...   4.800000   0.000000  0.200   \n",
       "1345   296.500000  126.500000  1236.000000  ...   2.300000   0.000000  0.200   \n",
       "1373    91.000000   50.000000   627.000000  ...   8.300000   1.200000  0.500   \n",
       "1402    46.000000  108.000000   255.000000  ...   7.000000   0.200000  0.300   \n",
       "1404    30.000000   19.000000   248.000000  ...   9.950000   0.500000  0.250   \n",
       "1517    44.000000   55.000000   448.000000  ...   1.933333   0.000000  0.100   \n",
       "1570    24.500000   26.000000   272.500000  ...   5.350000   3.150000  0.550   \n",
       "1581    31.500000   22.000000   311.500000  ...   3.100000   0.050000  0.150   \n",
       "1585    30.500000   15.000000   301.000000  ...   7.200000   1.400000  0.200   \n",
       "1586    18.500000   22.500000   268.000000  ...   6.950000   0.000000  0.300   \n",
       "\n",
       "            NET       LYT       MOT   EOT       BAT  Suspect  target  \n",
       "13    15.850000  0.200000  1.250000  0.00  0.000000      0.5       1  \n",
       "31    11.300000  0.400000  0.500000  0.00  0.000000      1.0       1  \n",
       "84    10.700000  1.500000  1.400000  0.00  0.000000      0.5       0  \n",
       "95     7.100000  3.400000  2.000000  0.00  0.000000      1.0       1  \n",
       "107    9.500000  0.300000  0.900000  0.00  0.000000      1.0       1  \n",
       "111    6.733333  0.566667  0.233333  0.00  0.000000      1.0       1  \n",
       "124    8.750000  4.600000  0.900000  0.10  0.050000      1.0       0  \n",
       "125   13.200000  2.000000  0.900000  0.00  0.000000      1.0       1  \n",
       "131    3.600000  2.300000  1.000000  0.10  0.100000      0.5       0  \n",
       "150   10.400000  1.100000  0.600000  0.10  0.000000      0.5       1  \n",
       "241    7.400000  0.900000  0.200000  0.00  0.050000      1.0       1  \n",
       "257   10.900000  1.550000  0.850000  0.00  0.000000      1.0       1  \n",
       "269   12.000000  1.100000  1.400000  0.00  0.000000      0.5       1  \n",
       "277    9.800000  1.250000  0.800000  0.00  0.000000      1.0       0  \n",
       "331    3.500000  1.100000  0.800000  0.20  0.000000      1.0       1  \n",
       "402    3.100000  1.700000  0.350000  0.00  0.000000      1.0       1  \n",
       "410    9.300000  0.700000  0.500000  0.00  0.000000      1.0       0  \n",
       "411   10.500000  0.800000  1.000000  0.00  0.000000      1.0       1  \n",
       "437   13.300000  0.200000  1.050000  0.00  0.050000      1.0       0  \n",
       "441    2.300000  0.900000  0.200000  0.00  0.000000      1.0       1  \n",
       "444   10.000000  0.550000  0.350000  0.00  0.000000      1.0       1  \n",
       "471    4.700000  0.200000  0.300000  0.00  0.000000      1.0       1  \n",
       "500    6.800000  0.850000  0.350000  0.00  0.000000      1.0       1  \n",
       "524   10.400000  0.750000  0.400000  0.10  0.000000      1.0       1  \n",
       "572    7.400000  0.600000  0.450000  0.00  0.000000      1.0       1  \n",
       "700    7.700000  1.200000  0.800000  0.00  0.000000      1.0       1  \n",
       "740    9.300000  1.400000  0.800000  0.10  0.000000      1.0       0  \n",
       "799    6.700000  2.300000  1.600000  1.30  0.100000      1.0       0  \n",
       "862   13.600000  0.300000  0.400000  0.00  0.000000      1.0       1  \n",
       "874    6.400000  0.600000  0.800000  0.00  0.000000      1.0       1  \n",
       "880    9.700000  0.900000  0.300000  0.00  0.100000      0.5       0  \n",
       "921    9.833333  0.700000  0.566667  0.00  0.033333      1.0       1  \n",
       "930   10.400000  1.700000  0.200000  0.00  0.000000      0.5       1  \n",
       "1025   8.600000  0.800000  1.100000  0.00  0.000000      1.0       1  \n",
       "1070   2.300000  1.500000  0.300000  0.10  0.000000      0.5       1  \n",
       "1073   8.450000  0.600000  0.950000  0.00  0.000000      1.0       1  \n",
       "1081   6.350000  0.550000  0.150000  0.00  0.000000      0.0       1  \n",
       "1135  17.350000  2.950000  1.050000  0.00  0.000000      1.0       1  \n",
       "1153   9.833333  0.700000  0.566667  0.00  0.033333      1.0       1  \n",
       "1169  11.900000  0.700000  0.875000  0.00  0.000000      1.0       0  \n",
       "1215   4.300000  0.900000  0.100000  0.00  0.000000      1.0       1  \n",
       "1221  11.200000  2.050000  0.450000  0.00  0.050000      1.0       1  \n",
       "1225  17.100000  2.800000  1.900000  0.00  0.100000      1.0       1  \n",
       "1258   5.800000  0.200000  1.300000  0.10  0.000000      0.5       0  \n",
       "1272   4.300000  1.050000  0.350000  0.00  0.000000      1.0       1  \n",
       "1291  10.200000  0.800000  0.600000  0.00  0.000000      1.0       1  \n",
       "1345  11.700000  0.700000  0.300000  0.00  0.000000      1.0       1  \n",
       "1373   4.900000  0.900000  0.500000  0.10  0.000000      1.0       1  \n",
       "1402  11.700000  1.100000  1.000000  0.00  0.000000      0.5       0  \n",
       "1404   5.150000  1.750000  0.800000  0.05  0.000000      0.0       0  \n",
       "1517  20.833333  0.500000  0.400000  0.00  0.000000      0.0       1  \n",
       "1570   8.850000  2.900000  0.650000  0.40  0.050000      0.0       0  \n",
       "1581  11.150000  0.500000  0.400000  0.00  0.000000      1.0       0  \n",
       "1585  13.500000  1.200000  1.200000  0.20  0.000000      1.0       0  \n",
       "1586  16.300000  2.250000  1.400000  0.00  0.100000      0.0       0  \n",
       "\n",
       "[55 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"Age\"]>20)&(df[\"GLU\"]>200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bff270",
   "metadata": {},
   "source": [
    "The 55 peoples having age greater than 20 and having high glucose level. i.e. >200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f1ccc",
   "metadata": {},
   "source": [
    "#### what is high and Low HB value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6c019cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.55"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"HGB\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bec5ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.65"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"HGB\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10165",
   "metadata": {},
   "source": [
    "Acoording to aur data the highest value of Hb is 18.55 and Lowest is 6.65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162e99b",
   "metadata": {},
   "source": [
    "#### How many childerns are admitted in hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c0c387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.283333</td>\n",
       "      <td>1.056667</td>\n",
       "      <td>61.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>87.5</td>\n",
       "      <td>28.333333</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>234.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>13.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.633333</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age        CA      CREA   ALP   GGT   GLU        AST        ALT  \\\n",
       "1391  1.0  17.0  2.283333  1.056667  61.0  36.5  87.5  28.333333  45.333333   \n",
       "\n",
       "             LDH  ...         MO        EO        BA  NET       LYT  MOT  EOT  \\\n",
       "1391  234.666667  ...  13.133333  0.066667  0.333333  7.6  1.633333  1.4  0.0   \n",
       "\n",
       "      BAT  Suspect  target  \n",
       "1391  0.0      0.0       1  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Age\"]<18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ee980",
   "metadata": {},
   "source": [
    "The only one child was admitted in Hospital during COVID 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e51d0",
   "metadata": {},
   "source": [
    "#### How many people are infected with covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c7737e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>364.5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>47.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>356.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>159.5</td>\n",
       "      <td>147.0</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>559.500000</td>\n",
       "      <td>694.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>15.850000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>775.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>268.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.961070</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>185.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.44</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>3.986000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.923645</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>44.0</td>\n",
       "      <td>206.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>422.3</td>\n",
       "      <td>...</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>11.664000</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.411833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.946667</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.012633</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.333333</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>242.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.886667</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.129067</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>81.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>190.666667</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>303.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>3.976667</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age        CA      CREA    ALP    GGT         GLU          AST  \\\n",
       "5     1.0  84.0  2.060000  1.283333   75.0   75.0   95.500000    40.500000   \n",
       "9     0.0  67.0  1.980000  0.610000   47.0   23.0  106.000000    54.000000   \n",
       "13    0.0  76.0  2.200000  2.420000  159.5  147.0  640.000000  1018.500000   \n",
       "18    1.0  60.0  2.100000  2.380000   50.0   20.0  154.000000   131.000000   \n",
       "23    1.0  85.0  1.940000  0.900000   44.0   33.0  101.000000    50.000000   \n",
       "...   ...   ...       ...       ...    ...    ...         ...          ...   \n",
       "1644  1.0  68.0  1.961070  0.688000   58.0   21.0   77.800000    12.200000   \n",
       "1650  0.0  79.0  1.923645  0.509000   44.0  206.6   93.000000    46.300000   \n",
       "1652  0.0  76.0  2.411833  0.750000   51.0   32.0   90.000000    25.333333   \n",
       "1678  1.0  58.0  2.012633  0.736667   42.0   20.0  100.333333    21.000000   \n",
       "1681  0.0  78.0  2.129067  0.483333   81.0   33.0  190.666667    21.000000   \n",
       "\n",
       "             ALT    LDH  ...     MO        EO        BA        NET       LYT  \\\n",
       "5      27.000000  364.5  ...   6.80  1.100000  0.600000   2.800000  0.600000   \n",
       "9      27.000000  356.0  ...   4.90  0.000000  0.000000   3.400000  0.700000   \n",
       "13    559.500000  694.0  ...   7.25  0.050000  0.050000  15.850000  0.200000   \n",
       "18     28.000000  775.0  ...   5.50  0.000000  0.200000  10.200000  0.800000   \n",
       "23     33.000000  268.0  ...   5.00  0.000000  0.000000   4.800000  0.600000   \n",
       "...          ...    ...  ...    ...       ...       ...        ...       ...   \n",
       "1644   16.400000  185.0  ...   7.44  1.320000  0.380000   3.986000  1.050000   \n",
       "1650   33.700000  422.3  ...   4.30  2.100000  0.170000  11.664000  0.895000   \n",
       "1652   58.000000  233.0  ...   8.00  2.200000  0.766667   2.946667  1.363333   \n",
       "1678   35.000000  242.5  ...  11.80  1.366667  0.100000   1.886667  1.116667   \n",
       "1681   34.666667  303.0  ...  10.00  3.866667  0.733333   3.976667  1.560000   \n",
       "\n",
       "           MOT       EOT       BAT  Suspect  target  \n",
       "5     0.250000  0.050000  0.000000      0.0       1  \n",
       "9     0.200000  0.000000  0.000000      1.0       1  \n",
       "13    1.250000  0.000000  0.000000      0.5       1  \n",
       "18    0.600000  0.000000  0.000000      1.0       1  \n",
       "23    0.300000  0.000000  0.000000      1.0       1  \n",
       "...        ...       ...       ...      ...     ...  \n",
       "1644  0.404000  0.072000  0.022000      1.0       1  \n",
       "1650  0.577000  0.286000  0.025000      1.0       1  \n",
       "1652  0.386667  0.106667  0.036667      1.0       1  \n",
       "1678  0.410000  0.050000  0.003333      1.0       1  \n",
       "1681  0.646667  0.246667  0.046667      1.0       1  \n",
       "\n",
       "[463 rows x 34 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"target\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9cb243",
   "metadata": {},
   "source": [
    "The 463 peoples are infected with Covid 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6ce35",
   "metadata": {},
   "source": [
    "### Out of all the pts how many are infected ,what was the precentage of male and female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5261b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "df[df[\"target\"]==1][\"Sex\"].value_counts().plot.pie(autopct=\"%1.2f%%\",labels=[\"Male\",\"Female\"])\n",
    "plt.title(\"How many PTs are infected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac122ef6",
   "metadata": {},
   "source": [
    "The pie chart shows the count in percent of male and female infected through covid 19 in that Male count is 61.99% and Female count is 38.01%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd2b8a",
   "metadata": {},
   "source": [
    "#### How many people may infected by covid on the basis of low HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81113cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[(df[\"HGB\"]>10)&(df[\"target\"]==1)][\"Sex\"].value_counts().plot.bar(\"HGB\",\"Target\")\n",
    "plt.title(\"Infected by covid due to low HB\")\n",
    "plt.xticks(rotation=360,ticks=[0,1],labels=[\"Male\",\"Female\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170208ce",
   "metadata": {},
   "source": [
    " The Bar graph shows there is 300 Male and 150 Females are infected with COVID 19 and having low HB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21c985",
   "metadata": {},
   "source": [
    "#### How many Suspects their"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7d07801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    631\n",
       "0.5    125\n",
       "0.0     70\n",
       "Name: Suspect, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Suspect\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ff9ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "df[\"Suspect\"].value_counts().plot.pie(autopct=\"%1.1f%%\")\n",
    "plt.title(\"Suspect_counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885f4d3",
   "metadata": {},
   "source": [
    "The piechart shows there is 76.4% peoples having severely ill pts, 15.1% peoples are in midstage and 8.5% of peoples having starting of symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92b8c",
   "metadata": {},
   "source": [
    "#### To find Correlation between each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd3a69aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.032964</td>\n",
       "      <td>-0.176472</td>\n",
       "      <td>0.137660</td>\n",
       "      <td>-0.047572</td>\n",
       "      <td>0.058831</td>\n",
       "      <td>0.032916</td>\n",
       "      <td>0.078685</td>\n",
       "      <td>0.093531</td>\n",
       "      <td>0.136243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017811</td>\n",
       "      <td>-0.046167</td>\n",
       "      <td>-0.063545</td>\n",
       "      <td>0.073384</td>\n",
       "      <td>-0.130488</td>\n",
       "      <td>0.003576</td>\n",
       "      <td>-0.065802</td>\n",
       "      <td>-0.080695</td>\n",
       "      <td>0.021589</td>\n",
       "      <td>0.126874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.032964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.238859</td>\n",
       "      <td>0.273081</td>\n",
       "      <td>0.120022</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>0.249804</td>\n",
       "      <td>0.115363</td>\n",
       "      <td>-0.041111</td>\n",
       "      <td>0.217702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>-0.186548</td>\n",
       "      <td>-0.178190</td>\n",
       "      <td>0.259953</td>\n",
       "      <td>-0.245906</td>\n",
       "      <td>0.106989</td>\n",
       "      <td>-0.144080</td>\n",
       "      <td>-0.034761</td>\n",
       "      <td>-0.158580</td>\n",
       "      <td>0.044156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>-0.176472</td>\n",
       "      <td>-0.238859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051125</td>\n",
       "      <td>-0.038610</td>\n",
       "      <td>-0.134819</td>\n",
       "      <td>-0.145684</td>\n",
       "      <td>-0.186710</td>\n",
       "      <td>-0.079054</td>\n",
       "      <td>-0.363870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123827</td>\n",
       "      <td>0.264232</td>\n",
       "      <td>0.333534</td>\n",
       "      <td>-0.055236</td>\n",
       "      <td>0.326983</td>\n",
       "      <td>0.074064</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>0.199333</td>\n",
       "      <td>-0.104685</td>\n",
       "      <td>-0.379474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CREA</th>\n",
       "      <td>0.137660</td>\n",
       "      <td>0.273081</td>\n",
       "      <td>-0.051125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>-0.047184</td>\n",
       "      <td>0.144224</td>\n",
       "      <td>0.114154</td>\n",
       "      <td>-0.021936</td>\n",
       "      <td>0.105350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068322</td>\n",
       "      <td>-0.031158</td>\n",
       "      <td>-0.101453</td>\n",
       "      <td>0.159411</td>\n",
       "      <td>-0.142916</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>-0.023793</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>-0.081629</td>\n",
       "      <td>-0.047042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALP</th>\n",
       "      <td>-0.047572</td>\n",
       "      <td>0.120022</td>\n",
       "      <td>-0.038610</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.627228</td>\n",
       "      <td>0.105397</td>\n",
       "      <td>0.163809</td>\n",
       "      <td>0.308004</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023203</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>0.300469</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>0.094119</td>\n",
       "      <td>0.044522</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>-0.127160</td>\n",
       "      <td>-0.099648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GGT</th>\n",
       "      <td>0.058831</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>-0.134819</td>\n",
       "      <td>-0.047184</td>\n",
       "      <td>0.627228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043929</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.405130</td>\n",
       "      <td>0.159610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.060766</td>\n",
       "      <td>-0.059102</td>\n",
       "      <td>0.091536</td>\n",
       "      <td>-0.052327</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>-0.059291</td>\n",
       "      <td>-0.011792</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>0.053349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLU</th>\n",
       "      <td>0.032916</td>\n",
       "      <td>0.249804</td>\n",
       "      <td>-0.145684</td>\n",
       "      <td>0.144224</td>\n",
       "      <td>0.105397</td>\n",
       "      <td>0.043929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282561</td>\n",
       "      <td>0.151524</td>\n",
       "      <td>0.218363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079999</td>\n",
       "      <td>-0.104527</td>\n",
       "      <td>-0.145554</td>\n",
       "      <td>0.212451</td>\n",
       "      <td>-0.107450</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>-0.066400</td>\n",
       "      <td>-0.047000</td>\n",
       "      <td>-0.014618</td>\n",
       "      <td>0.091141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AST</th>\n",
       "      <td>0.078685</td>\n",
       "      <td>0.115363</td>\n",
       "      <td>-0.186710</td>\n",
       "      <td>0.114154</td>\n",
       "      <td>0.163809</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.282561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795529</td>\n",
       "      <td>0.521652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069567</td>\n",
       "      <td>-0.116296</td>\n",
       "      <td>-0.130645</td>\n",
       "      <td>0.122755</td>\n",
       "      <td>-0.109277</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>-0.108125</td>\n",
       "      <td>-0.073203</td>\n",
       "      <td>0.030381</td>\n",
       "      <td>0.203083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALT</th>\n",
       "      <td>0.093531</td>\n",
       "      <td>-0.041111</td>\n",
       "      <td>-0.079054</td>\n",
       "      <td>-0.021936</td>\n",
       "      <td>0.308004</td>\n",
       "      <td>0.405130</td>\n",
       "      <td>0.151524</td>\n",
       "      <td>0.795529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042104</td>\n",
       "      <td>-0.037094</td>\n",
       "      <td>-0.024478</td>\n",
       "      <td>0.104278</td>\n",
       "      <td>-0.046293</td>\n",
       "      <td>-0.008331</td>\n",
       "      <td>-0.048836</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.142488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDH</th>\n",
       "      <td>0.136243</td>\n",
       "      <td>0.217702</td>\n",
       "      <td>-0.363870</td>\n",
       "      <td>0.105350</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.159610</td>\n",
       "      <td>0.218363</td>\n",
       "      <td>0.521652</td>\n",
       "      <td>0.323935</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160423</td>\n",
       "      <td>-0.239550</td>\n",
       "      <td>-0.261691</td>\n",
       "      <td>0.260945</td>\n",
       "      <td>-0.233573</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>-0.197901</td>\n",
       "      <td>-0.112149</td>\n",
       "      <td>0.145050</td>\n",
       "      <td>0.407378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCR</th>\n",
       "      <td>0.168879</td>\n",
       "      <td>0.271573</td>\n",
       "      <td>-0.370845</td>\n",
       "      <td>0.137722</td>\n",
       "      <td>0.149641</td>\n",
       "      <td>0.136683</td>\n",
       "      <td>0.201261</td>\n",
       "      <td>0.171029</td>\n",
       "      <td>0.088116</td>\n",
       "      <td>0.486546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279232</td>\n",
       "      <td>-0.248449</td>\n",
       "      <td>-0.281204</td>\n",
       "      <td>0.382172</td>\n",
       "      <td>-0.331172</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>-0.216464</td>\n",
       "      <td>-0.072307</td>\n",
       "      <td>0.116676</td>\n",
       "      <td>0.258966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KAL</th>\n",
       "      <td>0.065228</td>\n",
       "      <td>0.137365</td>\n",
       "      <td>0.099978</td>\n",
       "      <td>0.349690</td>\n",
       "      <td>0.060161</td>\n",
       "      <td>-0.038302</td>\n",
       "      <td>0.198980</td>\n",
       "      <td>0.059899</td>\n",
       "      <td>0.008268</td>\n",
       "      <td>0.045617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.044742</td>\n",
       "      <td>0.043363</td>\n",
       "      <td>0.124954</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.057370</td>\n",
       "      <td>0.084973</td>\n",
       "      <td>-0.061913</td>\n",
       "      <td>-0.056834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAT</th>\n",
       "      <td>-0.068235</td>\n",
       "      <td>0.119231</td>\n",
       "      <td>0.186559</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>-0.081768</td>\n",
       "      <td>-0.044650</td>\n",
       "      <td>-0.008688</td>\n",
       "      <td>-0.063149</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066392</td>\n",
       "      <td>0.061778</td>\n",
       "      <td>0.069474</td>\n",
       "      <td>0.012202</td>\n",
       "      <td>0.113930</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>0.033422</td>\n",
       "      <td>0.081822</td>\n",
       "      <td>-0.074148</td>\n",
       "      <td>-0.059417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UREA</th>\n",
       "      <td>0.090792</td>\n",
       "      <td>0.471753</td>\n",
       "      <td>-0.052998</td>\n",
       "      <td>0.779750</td>\n",
       "      <td>0.023025</td>\n",
       "      <td>-0.024742</td>\n",
       "      <td>0.264762</td>\n",
       "      <td>0.190825</td>\n",
       "      <td>0.008988</td>\n",
       "      <td>0.252722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114113</td>\n",
       "      <td>-0.117564</td>\n",
       "      <td>-0.160639</td>\n",
       "      <td>0.272529</td>\n",
       "      <td>-0.150993</td>\n",
       "      <td>0.079727</td>\n",
       "      <td>-0.098464</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>-0.080411</td>\n",
       "      <td>0.015541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBC</th>\n",
       "      <td>0.044195</td>\n",
       "      <td>0.211054</td>\n",
       "      <td>0.024329</td>\n",
       "      <td>0.126150</td>\n",
       "      <td>0.295461</td>\n",
       "      <td>0.076712</td>\n",
       "      <td>0.192364</td>\n",
       "      <td>0.090159</td>\n",
       "      <td>0.082566</td>\n",
       "      <td>0.202098</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210900</td>\n",
       "      <td>-0.055732</td>\n",
       "      <td>-0.086165</td>\n",
       "      <td>0.959555</td>\n",
       "      <td>0.196013</td>\n",
       "      <td>0.418829</td>\n",
       "      <td>0.065862</td>\n",
       "      <td>0.347314</td>\n",
       "      <td>-0.144786</td>\n",
       "      <td>-0.197148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBC</th>\n",
       "      <td>0.171936</td>\n",
       "      <td>-0.364782</td>\n",
       "      <td>0.223227</td>\n",
       "      <td>-0.267463</td>\n",
       "      <td>-0.205150</td>\n",
       "      <td>-0.081024</td>\n",
       "      <td>-0.099958</td>\n",
       "      <td>-0.004908</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.029211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032408</td>\n",
       "      <td>0.026968</td>\n",
       "      <td>0.039746</td>\n",
       "      <td>-0.159772</td>\n",
       "      <td>0.133572</td>\n",
       "      <td>-0.083828</td>\n",
       "      <td>0.032513</td>\n",
       "      <td>-0.026683</td>\n",
       "      <td>0.171780</td>\n",
       "      <td>0.164482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HGB</th>\n",
       "      <td>0.245951</td>\n",
       "      <td>-0.284492</td>\n",
       "      <td>0.216787</td>\n",
       "      <td>-0.242544</td>\n",
       "      <td>-0.212218</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>-0.108397</td>\n",
       "      <td>-0.011968</td>\n",
       "      <td>0.073541</td>\n",
       "      <td>0.045825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.072164</td>\n",
       "      <td>-0.168182</td>\n",
       "      <td>0.149412</td>\n",
       "      <td>-0.064510</td>\n",
       "      <td>0.018472</td>\n",
       "      <td>-0.008375</td>\n",
       "      <td>0.163932</td>\n",
       "      <td>0.169805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HCT</th>\n",
       "      <td>0.216683</td>\n",
       "      <td>-0.243641</td>\n",
       "      <td>0.236125</td>\n",
       "      <td>-0.227804</td>\n",
       "      <td>-0.199339</td>\n",
       "      <td>-0.043371</td>\n",
       "      <td>-0.078642</td>\n",
       "      <td>-0.010606</td>\n",
       "      <td>0.061446</td>\n",
       "      <td>0.054872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046662</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>-0.145532</td>\n",
       "      <td>0.152796</td>\n",
       "      <td>-0.049258</td>\n",
       "      <td>0.034691</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.148398</td>\n",
       "      <td>0.162012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCV</th>\n",
       "      <td>0.051476</td>\n",
       "      <td>0.300263</td>\n",
       "      <td>-0.029121</td>\n",
       "      <td>0.139621</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.089164</td>\n",
       "      <td>0.053629</td>\n",
       "      <td>-0.013130</td>\n",
       "      <td>-0.010722</td>\n",
       "      <td>0.037602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025290</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.062080</td>\n",
       "      <td>0.059722</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.093663</td>\n",
       "      <td>-0.009071</td>\n",
       "      <td>0.061686</td>\n",
       "      <td>-0.083632</td>\n",
       "      <td>-0.048345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCH</th>\n",
       "      <td>0.119644</td>\n",
       "      <td>0.150609</td>\n",
       "      <td>-0.025029</td>\n",
       "      <td>0.053350</td>\n",
       "      <td>-0.011921</td>\n",
       "      <td>0.069229</td>\n",
       "      <td>-0.019143</td>\n",
       "      <td>-0.021760</td>\n",
       "      <td>0.012669</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010750</td>\n",
       "      <td>-0.009050</td>\n",
       "      <td>0.055993</td>\n",
       "      <td>-0.015755</td>\n",
       "      <td>0.023535</td>\n",
       "      <td>0.045333</td>\n",
       "      <td>-0.028156</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>-0.021871</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCHC</th>\n",
       "      <td>0.174265</td>\n",
       "      <td>-0.214973</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>-0.129922</td>\n",
       "      <td>-0.123416</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>-0.141899</td>\n",
       "      <td>-0.026832</td>\n",
       "      <td>0.046994</td>\n",
       "      <td>-0.008371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028263</td>\n",
       "      <td>-0.030822</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>-0.141850</td>\n",
       "      <td>0.048445</td>\n",
       "      <td>-0.074704</td>\n",
       "      <td>-0.046640</td>\n",
       "      <td>-0.052358</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.095246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLT1</th>\n",
       "      <td>-0.089843</td>\n",
       "      <td>-0.120709</td>\n",
       "      <td>0.150566</td>\n",
       "      <td>-0.105594</td>\n",
       "      <td>0.147107</td>\n",
       "      <td>0.018547</td>\n",
       "      <td>0.103440</td>\n",
       "      <td>-0.007371</td>\n",
       "      <td>0.065374</td>\n",
       "      <td>0.030799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100262</td>\n",
       "      <td>0.103410</td>\n",
       "      <td>0.132102</td>\n",
       "      <td>0.244783</td>\n",
       "      <td>0.208791</td>\n",
       "      <td>0.085191</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.219490</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>-0.103297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>0.145248</td>\n",
       "      <td>0.365123</td>\n",
       "      <td>-0.302359</td>\n",
       "      <td>0.192153</td>\n",
       "      <td>0.102923</td>\n",
       "      <td>0.049562</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.162003</td>\n",
       "      <td>0.065528</td>\n",
       "      <td>0.372021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.551813</td>\n",
       "      <td>-0.422977</td>\n",
       "      <td>-0.427724</td>\n",
       "      <td>0.565045</td>\n",
       "      <td>-0.621930</td>\n",
       "      <td>-0.114371</td>\n",
       "      <td>-0.310431</td>\n",
       "      <td>-0.132704</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.145222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LY</th>\n",
       "      <td>-0.175347</td>\n",
       "      <td>-0.390806</td>\n",
       "      <td>0.276146</td>\n",
       "      <td>-0.204270</td>\n",
       "      <td>-0.119994</td>\n",
       "      <td>-0.045799</td>\n",
       "      <td>-0.250950</td>\n",
       "      <td>-0.151572</td>\n",
       "      <td>-0.057770</td>\n",
       "      <td>-0.355357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282984</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.349584</td>\n",
       "      <td>-0.546142</td>\n",
       "      <td>0.670796</td>\n",
       "      <td>-0.077999</td>\n",
       "      <td>0.186480</td>\n",
       "      <td>0.088337</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>-0.113462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MO</th>\n",
       "      <td>0.017811</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.123827</td>\n",
       "      <td>-0.068322</td>\n",
       "      <td>-0.023203</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.079999</td>\n",
       "      <td>-0.069567</td>\n",
       "      <td>-0.042104</td>\n",
       "      <td>-0.160423</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153970</td>\n",
       "      <td>0.214317</td>\n",
       "      <td>-0.321539</td>\n",
       "      <td>0.107308</td>\n",
       "      <td>0.568835</td>\n",
       "      <td>0.100916</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>-0.015085</td>\n",
       "      <td>-0.030226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EO</th>\n",
       "      <td>-0.046167</td>\n",
       "      <td>-0.186548</td>\n",
       "      <td>0.264232</td>\n",
       "      <td>-0.031158</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>-0.060766</td>\n",
       "      <td>-0.104527</td>\n",
       "      <td>-0.116296</td>\n",
       "      <td>-0.037094</td>\n",
       "      <td>-0.239550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.441557</td>\n",
       "      <td>-0.153579</td>\n",
       "      <td>0.279187</td>\n",
       "      <td>0.062913</td>\n",
       "      <td>0.926881</td>\n",
       "      <td>0.329643</td>\n",
       "      <td>-0.051856</td>\n",
       "      <td>-0.288397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>-0.063545</td>\n",
       "      <td>-0.178190</td>\n",
       "      <td>0.333534</td>\n",
       "      <td>-0.101453</td>\n",
       "      <td>-0.001227</td>\n",
       "      <td>-0.059102</td>\n",
       "      <td>-0.145554</td>\n",
       "      <td>-0.130645</td>\n",
       "      <td>-0.024478</td>\n",
       "      <td>-0.261691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214317</td>\n",
       "      <td>0.441557</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.174432</td>\n",
       "      <td>0.302054</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>0.398641</td>\n",
       "      <td>0.615172</td>\n",
       "      <td>-0.129686</td>\n",
       "      <td>-0.333464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NET</th>\n",
       "      <td>0.073384</td>\n",
       "      <td>0.259953</td>\n",
       "      <td>-0.055236</td>\n",
       "      <td>0.159411</td>\n",
       "      <td>0.300469</td>\n",
       "      <td>0.091536</td>\n",
       "      <td>0.212451</td>\n",
       "      <td>0.122755</td>\n",
       "      <td>0.104278</td>\n",
       "      <td>0.260945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321539</td>\n",
       "      <td>-0.153579</td>\n",
       "      <td>-0.174432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019872</td>\n",
       "      <td>0.266103</td>\n",
       "      <td>-0.037089</td>\n",
       "      <td>0.258222</td>\n",
       "      <td>-0.130560</td>\n",
       "      <td>-0.125708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LYT</th>\n",
       "      <td>-0.130488</td>\n",
       "      <td>-0.245906</td>\n",
       "      <td>0.326983</td>\n",
       "      <td>-0.142916</td>\n",
       "      <td>-0.005195</td>\n",
       "      <td>-0.052327</td>\n",
       "      <td>-0.107450</td>\n",
       "      <td>-0.109277</td>\n",
       "      <td>-0.046293</td>\n",
       "      <td>-0.233573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107308</td>\n",
       "      <td>0.279187</td>\n",
       "      <td>0.302054</td>\n",
       "      <td>-0.019872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>0.308263</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>-0.033375</td>\n",
       "      <td>-0.267315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOT</th>\n",
       "      <td>0.003576</td>\n",
       "      <td>0.106989</td>\n",
       "      <td>0.074064</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.094119</td>\n",
       "      <td>0.009207</td>\n",
       "      <td>0.106548</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>-0.008331</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568835</td>\n",
       "      <td>0.062913</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>0.266103</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>-0.060160</td>\n",
       "      <td>-0.118688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EOT</th>\n",
       "      <td>-0.065802</td>\n",
       "      <td>-0.144080</td>\n",
       "      <td>0.252001</td>\n",
       "      <td>-0.023793</td>\n",
       "      <td>0.044522</td>\n",
       "      <td>-0.059291</td>\n",
       "      <td>-0.066400</td>\n",
       "      <td>-0.108125</td>\n",
       "      <td>-0.048836</td>\n",
       "      <td>-0.197901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100916</td>\n",
       "      <td>0.926881</td>\n",
       "      <td>0.398641</td>\n",
       "      <td>-0.037089</td>\n",
       "      <td>0.308263</td>\n",
       "      <td>0.109259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391256</td>\n",
       "      <td>-0.063500</td>\n",
       "      <td>-0.289252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAT</th>\n",
       "      <td>-0.080695</td>\n",
       "      <td>-0.034761</td>\n",
       "      <td>0.199333</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>-0.011792</td>\n",
       "      <td>-0.047000</td>\n",
       "      <td>-0.073203</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>-0.112149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011074</td>\n",
       "      <td>0.329643</td>\n",
       "      <td>0.615172</td>\n",
       "      <td>0.258222</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>0.215145</td>\n",
       "      <td>0.391256</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.155561</td>\n",
       "      <td>-0.302523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Suspect</th>\n",
       "      <td>0.021589</td>\n",
       "      <td>-0.158580</td>\n",
       "      <td>-0.104685</td>\n",
       "      <td>-0.081629</td>\n",
       "      <td>-0.127160</td>\n",
       "      <td>0.015321</td>\n",
       "      <td>-0.014618</td>\n",
       "      <td>0.030381</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.145050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015085</td>\n",
       "      <td>-0.051856</td>\n",
       "      <td>-0.129686</td>\n",
       "      <td>-0.130560</td>\n",
       "      <td>-0.033375</td>\n",
       "      <td>-0.060160</td>\n",
       "      <td>-0.063500</td>\n",
       "      <td>-0.155561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.126874</td>\n",
       "      <td>0.044156</td>\n",
       "      <td>-0.379474</td>\n",
       "      <td>-0.047042</td>\n",
       "      <td>-0.099648</td>\n",
       "      <td>0.053349</td>\n",
       "      <td>0.091141</td>\n",
       "      <td>0.203083</td>\n",
       "      <td>0.142488</td>\n",
       "      <td>0.407378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030226</td>\n",
       "      <td>-0.288397</td>\n",
       "      <td>-0.333464</td>\n",
       "      <td>-0.125708</td>\n",
       "      <td>-0.267315</td>\n",
       "      <td>-0.118688</td>\n",
       "      <td>-0.289252</td>\n",
       "      <td>-0.302523</td>\n",
       "      <td>0.284294</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sex       Age        CA      CREA       ALP       GGT       GLU  \\\n",
       "Sex      1.000000  0.032964 -0.176472  0.137660 -0.047572  0.058831  0.032916   \n",
       "Age      0.032964  1.000000 -0.238859  0.273081  0.120022  0.053419  0.249804   \n",
       "CA      -0.176472 -0.238859  1.000000 -0.051125 -0.038610 -0.134819 -0.145684   \n",
       "CREA     0.137660  0.273081 -0.051125  1.000000  0.002392 -0.047184  0.144224   \n",
       "ALP     -0.047572  0.120022 -0.038610  0.002392  1.000000  0.627228  0.105397   \n",
       "GGT      0.058831  0.053419 -0.134819 -0.047184  0.627228  1.000000  0.043929   \n",
       "GLU      0.032916  0.249804 -0.145684  0.144224  0.105397  0.043929  1.000000   \n",
       "AST      0.078685  0.115363 -0.186710  0.114154  0.163809  0.245873  0.282561   \n",
       "ALT      0.093531 -0.041111 -0.079054 -0.021936  0.308004  0.405130  0.151524   \n",
       "LDH      0.136243  0.217702 -0.363870  0.105350  0.067700  0.159610  0.218363   \n",
       "PCR      0.168879  0.271573 -0.370845  0.137722  0.149641  0.136683  0.201261   \n",
       "KAL      0.065228  0.137365  0.099978  0.349690  0.060161 -0.038302  0.198980   \n",
       "NAT     -0.068235  0.119231  0.186559  0.141179 -0.050146 -0.081768 -0.044650   \n",
       "UREA     0.090792  0.471753 -0.052998  0.779750  0.023025 -0.024742  0.264762   \n",
       "WBC      0.044195  0.211054  0.024329  0.126150  0.295461  0.076712  0.192364   \n",
       "RBC      0.171936 -0.364782  0.223227 -0.267463 -0.205150 -0.081024 -0.099958   \n",
       "HGB      0.245951 -0.284492  0.216787 -0.242544 -0.212218 -0.039495 -0.108397   \n",
       "HCT      0.216683 -0.243641  0.236125 -0.227804 -0.199339 -0.043371 -0.078642   \n",
       "MCV      0.051476  0.300263 -0.029121  0.139621  0.052662  0.089164  0.053629   \n",
       "MCH      0.119644  0.150609 -0.025029  0.053350 -0.011921  0.069229 -0.019143   \n",
       "MCHC     0.174265 -0.214973  0.003260 -0.129922 -0.123416 -0.006876 -0.141899   \n",
       "PLT1    -0.089843 -0.120709  0.150566 -0.105594  0.147107  0.018547  0.103440   \n",
       "NE       0.145248  0.365123 -0.302359  0.192153  0.102923  0.049562  0.243878   \n",
       "LY      -0.175347 -0.390806  0.276146 -0.204270 -0.119994 -0.045799 -0.250950   \n",
       "MO       0.017811 -0.066583  0.123827 -0.068322 -0.023203 -0.008706 -0.079999   \n",
       "EO      -0.046167 -0.186548  0.264232 -0.031158  0.023076 -0.060766 -0.104527   \n",
       "BA      -0.063545 -0.178190  0.333534 -0.101453 -0.001227 -0.059102 -0.145554   \n",
       "NET      0.073384  0.259953 -0.055236  0.159411  0.300469  0.091536  0.212451   \n",
       "LYT     -0.130488 -0.245906  0.326983 -0.142916 -0.005195 -0.052327 -0.107450   \n",
       "MOT      0.003576  0.106989  0.074064  0.016529  0.094119  0.009207  0.106548   \n",
       "EOT     -0.065802 -0.144080  0.252001 -0.023793  0.044522 -0.059291 -0.066400   \n",
       "BAT     -0.080695 -0.034761  0.199333  0.008746  0.095796 -0.011792 -0.047000   \n",
       "Suspect  0.021589 -0.158580 -0.104685 -0.081629 -0.127160  0.015321 -0.014618   \n",
       "target   0.126874  0.044156 -0.379474 -0.047042 -0.099648  0.053349  0.091141   \n",
       "\n",
       "              AST       ALT       LDH  ...        MO        EO        BA  \\\n",
       "Sex      0.078685  0.093531  0.136243  ...  0.017811 -0.046167 -0.063545   \n",
       "Age      0.115363 -0.041111  0.217702  ... -0.066583 -0.186548 -0.178190   \n",
       "CA      -0.186710 -0.079054 -0.363870  ...  0.123827  0.264232  0.333534   \n",
       "CREA     0.114154 -0.021936  0.105350  ... -0.068322 -0.031158 -0.101453   \n",
       "ALP      0.163809  0.308004  0.067700  ... -0.023203  0.023076 -0.001227   \n",
       "GGT      0.245873  0.405130  0.159610  ... -0.008706 -0.060766 -0.059102   \n",
       "GLU      0.282561  0.151524  0.218363  ... -0.079999 -0.104527 -0.145554   \n",
       "AST      1.000000  0.795529  0.521652  ... -0.069567 -0.116296 -0.130645   \n",
       "ALT      0.795529  1.000000  0.323935  ... -0.042104 -0.037094 -0.024478   \n",
       "LDH      0.521652  0.323935  1.000000  ... -0.160423 -0.239550 -0.261691   \n",
       "PCR      0.171029  0.088116  0.486546  ... -0.279232 -0.248449 -0.281204   \n",
       "KAL      0.059899  0.008268  0.045617  ...  0.046572  0.044742  0.043363   \n",
       "NAT     -0.008688 -0.063149  0.012716  ... -0.066392  0.061778  0.069474   \n",
       "UREA     0.190825  0.008988  0.252722  ... -0.114113 -0.117564 -0.160639   \n",
       "WBC      0.090159  0.082566  0.202098  ... -0.210900 -0.055732 -0.086165   \n",
       "RBC     -0.004908  0.055650  0.029211  ...  0.032408  0.026968  0.039746   \n",
       "HGB     -0.011968  0.073541  0.045825  ...  0.037254  0.021182  0.072164   \n",
       "HCT     -0.010606  0.061446  0.054872  ...  0.046662  0.033335  0.076279   \n",
       "MCV     -0.013130 -0.010722  0.037602  ...  0.025290  0.005259  0.062080   \n",
       "MCH     -0.021760  0.012669  0.023515  ...  0.010750 -0.009050  0.055993   \n",
       "MCHC    -0.026832  0.046994 -0.008371  ... -0.028263 -0.030822  0.009969   \n",
       "PLT1    -0.007371  0.065374  0.030799  ... -0.100262  0.103410  0.132102   \n",
       "NE       0.162003  0.065528  0.372021  ... -0.551813 -0.422977 -0.427724   \n",
       "LY      -0.151572 -0.057770 -0.355357  ...  0.282984  0.292453  0.349584   \n",
       "MO      -0.069567 -0.042104 -0.160423  ...  1.000000  0.153970  0.214317   \n",
       "EO      -0.116296 -0.037094 -0.239550  ...  0.153970  1.000000  0.441557   \n",
       "BA      -0.130645 -0.024478 -0.261691  ...  0.214317  0.441557  1.000000   \n",
       "NET      0.122755  0.104278  0.260945  ... -0.321539 -0.153579 -0.174432   \n",
       "LYT     -0.109277 -0.046293 -0.233573  ...  0.107308  0.279187  0.302054   \n",
       "MOT      0.025349 -0.008331  0.054467  ...  0.568835  0.062913  0.062142   \n",
       "EOT     -0.108125 -0.048836 -0.197901  ...  0.100916  0.926881  0.398641   \n",
       "BAT     -0.073203 -0.005159 -0.112149  ...  0.011074  0.329643  0.615172   \n",
       "Suspect  0.030381  0.006175  0.145050  ... -0.015085 -0.051856 -0.129686   \n",
       "target   0.203083  0.142488  0.407378  ... -0.030226 -0.288397 -0.333464   \n",
       "\n",
       "              NET       LYT       MOT       EOT       BAT   Suspect    target  \n",
       "Sex      0.073384 -0.130488  0.003576 -0.065802 -0.080695  0.021589  0.126874  \n",
       "Age      0.259953 -0.245906  0.106989 -0.144080 -0.034761 -0.158580  0.044156  \n",
       "CA      -0.055236  0.326983  0.074064  0.252001  0.199333 -0.104685 -0.379474  \n",
       "CREA     0.159411 -0.142916  0.016529 -0.023793  0.008746 -0.081629 -0.047042  \n",
       "ALP      0.300469 -0.005195  0.094119  0.044522  0.095796 -0.127160 -0.099648  \n",
       "GGT      0.091536 -0.052327  0.009207 -0.059291 -0.011792  0.015321  0.053349  \n",
       "GLU      0.212451 -0.107450  0.106548 -0.066400 -0.047000 -0.014618  0.091141  \n",
       "AST      0.122755 -0.109277  0.025349 -0.108125 -0.073203  0.030381  0.203083  \n",
       "ALT      0.104278 -0.046293 -0.008331 -0.048836 -0.005159  0.006175  0.142488  \n",
       "LDH      0.260945 -0.233573  0.054467 -0.197901 -0.112149  0.145050  0.407378  \n",
       "PCR      0.382172 -0.331172 -0.011935 -0.216464 -0.072307  0.116676  0.258966  \n",
       "KAL      0.124954  0.036032  0.034408  0.057370  0.084973 -0.061913 -0.056834  \n",
       "NAT      0.012202  0.113930 -0.004635  0.033422  0.081822 -0.074148 -0.059417  \n",
       "UREA     0.272529 -0.150993  0.079727 -0.098464  0.002795 -0.080411  0.015541  \n",
       "WBC      0.959555  0.196013  0.418829  0.065862  0.347314 -0.144786 -0.197148  \n",
       "RBC     -0.159772  0.133572 -0.083828  0.032513 -0.026683  0.171780  0.164482  \n",
       "HGB     -0.168182  0.149412 -0.064510  0.018472 -0.008375  0.163932  0.169805  \n",
       "HCT     -0.145532  0.152796 -0.049258  0.034691  0.007237  0.148398  0.162012  \n",
       "MCV      0.059722  0.001015  0.093663 -0.009071  0.061686 -0.083632 -0.048345  \n",
       "MCH     -0.015755  0.023535  0.045333 -0.028156  0.025442 -0.021871  0.001900  \n",
       "MCHC    -0.141850  0.048445 -0.074704 -0.046640 -0.052358  0.105469  0.095246  \n",
       "PLT1     0.244783  0.208791  0.085191  0.162800  0.219490  0.013052 -0.103297  \n",
       "NE       0.565045 -0.621930 -0.114371 -0.310431 -0.132704  0.006804  0.145222  \n",
       "LY      -0.546142  0.670796 -0.077999  0.186480  0.088337  0.008265 -0.113462  \n",
       "MO      -0.321539  0.107308  0.568835  0.100916  0.011074 -0.015085 -0.030226  \n",
       "EO      -0.153579  0.279187  0.062913  0.926881  0.329643 -0.051856 -0.288397  \n",
       "BA      -0.174432  0.302054  0.062142  0.398641  0.615172 -0.129686 -0.333464  \n",
       "NET      1.000000 -0.019872  0.266103 -0.037089  0.258222 -0.130560 -0.125708  \n",
       "LYT     -0.019872  1.000000  0.245989  0.308263  0.363922 -0.033375 -0.267315  \n",
       "MOT      0.266103  0.245989  1.000000  0.109259  0.215145 -0.060160 -0.118688  \n",
       "EOT     -0.037089  0.308263  0.109259  1.000000  0.391256 -0.063500 -0.289252  \n",
       "BAT      0.258222  0.363922  0.215145  0.391256  1.000000 -0.155561 -0.302523  \n",
       "Suspect -0.130560 -0.033375 -0.060160 -0.063500 -0.155561  1.000000  0.284294  \n",
       "target  -0.125708 -0.267315 -0.118688 -0.289252 -0.302523  0.284294  1.000000  \n",
       "\n",
       "[34 rows x 34 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b80ba",
   "metadata": {},
   "source": [
    "Data is less correlate with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18024086",
   "metadata": {},
   "source": [
    "#### To check skewness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e48d378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "      <td>826.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.564165</td>\n",
       "      <td>62.133172</td>\n",
       "      <td>2.182430</td>\n",
       "      <td>1.227921</td>\n",
       "      <td>87.857244</td>\n",
       "      <td>68.300363</td>\n",
       "      <td>121.118503</td>\n",
       "      <td>49.480529</td>\n",
       "      <td>41.570137</td>\n",
       "      <td>336.438862</td>\n",
       "      <td>...</td>\n",
       "      <td>7.557262</td>\n",
       "      <td>0.916408</td>\n",
       "      <td>0.347084</td>\n",
       "      <td>6.625512</td>\n",
       "      <td>1.345116</td>\n",
       "      <td>0.613654</td>\n",
       "      <td>0.070652</td>\n",
       "      <td>0.016383</td>\n",
       "      <td>0.839588</td>\n",
       "      <td>0.560533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496166</td>\n",
       "      <td>17.485903</td>\n",
       "      <td>0.174258</td>\n",
       "      <td>1.154152</td>\n",
       "      <td>72.718560</td>\n",
       "      <td>122.832409</td>\n",
       "      <td>58.842110</td>\n",
       "      <td>58.682990</td>\n",
       "      <td>44.362846</td>\n",
       "      <td>167.538487</td>\n",
       "      <td>...</td>\n",
       "      <td>3.923517</td>\n",
       "      <td>1.732202</td>\n",
       "      <td>0.269229</td>\n",
       "      <td>4.707825</td>\n",
       "      <td>0.884119</td>\n",
       "      <td>0.598457</td>\n",
       "      <td>0.158170</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.311391</td>\n",
       "      <td>0.496623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.762500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.175000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.475000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>72.875000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>403.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.475000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>780.500000</td>\n",
       "      <td>2272.000000</td>\n",
       "      <td>855.000000</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>559.500000</td>\n",
       "      <td>1690.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.700000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>10.150000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sex         Age          CA        CREA         ALP  \\\n",
       "count  826.000000  826.000000  826.000000  826.000000  826.000000   \n",
       "mean     0.564165   62.133172    2.182430    1.227921   87.857244   \n",
       "std      0.496166   17.485903    0.174258    1.154152   72.718560   \n",
       "min      0.000000   17.000000    1.363333    0.420000   23.000000   \n",
       "25%      0.000000   49.000000    2.070000    0.760000   56.000000   \n",
       "50%      1.000000   62.000000    2.170000    0.950000   70.500000   \n",
       "75%      1.000000   77.000000    2.290000    1.200000   93.000000   \n",
       "max      1.000000   99.000000    3.280000   15.700000  780.500000   \n",
       "\n",
       "               GGT         GLU          AST         ALT          LDH  ...  \\\n",
       "count   826.000000  826.000000   826.000000  826.000000   826.000000  ...   \n",
       "mean     68.300363  121.118503    49.480529   41.570137   336.438862  ...   \n",
       "std     122.832409   58.842110    58.682990   44.362846   167.538487  ...   \n",
       "min       4.000000   40.000000    12.000000    6.000000   104.000000  ...   \n",
       "25%      21.000000   92.000000    24.000000   19.000000   222.000000  ...   \n",
       "50%      35.000000  106.000000    34.000000   29.333333   291.000000  ...   \n",
       "75%      72.875000  127.000000    55.000000   48.750000   403.875000  ...   \n",
       "max    2272.000000  855.000000  1018.500000  559.500000  1690.500000  ...   \n",
       "\n",
       "               MO          EO          BA         NET         LYT         MOT  \\\n",
       "count  826.000000  826.000000  826.000000  826.000000  826.000000  826.000000   \n",
       "mean     7.557262    0.916408    0.347084    6.625512    1.345116    0.613654   \n",
       "std      3.923517    1.732202    0.269229    4.707825    0.884119    0.598457   \n",
       "min      0.200000    0.000000    0.000000    0.400000    0.000000    0.000000   \n",
       "25%      5.125000    0.000000    0.200000    3.762500    0.800000    0.400000   \n",
       "50%      7.175000    0.200000    0.300000    5.475000    1.100000    0.500000   \n",
       "75%      9.475000    1.100000    0.450000    8.250000    1.700000    0.800000   \n",
       "max     42.700000   12.700000    2.000000   73.000000   10.150000   14.500000   \n",
       "\n",
       "              EOT         BAT     Suspect      target  \n",
       "count  826.000000  826.000000  826.000000  826.000000  \n",
       "mean     0.070652    0.016383    0.839588    0.560533  \n",
       "std      0.158170    0.036600    0.311391    0.496623  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    1.000000    0.000000  \n",
       "50%      0.000000    0.000000    1.000000    1.000000  \n",
       "75%      0.100000    0.000000    1.000000    1.000000  \n",
       "max      1.450000    0.200000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd90ab2",
   "metadata": {},
   "source": [
    "#### Splitting Data Into Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "732f7e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>LDH</th>\n",
       "      <th>...</th>\n",
       "      <th>LY</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.400000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>123.5</td>\n",
       "      <td>176.5</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>9.50</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>264.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>364.5</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>189.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.961070</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>185.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.240000</td>\n",
       "      <td>7.44</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>3.986000</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.923645</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>44.0</td>\n",
       "      <td>206.6</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>33.700000</td>\n",
       "      <td>422.3</td>\n",
       "      <td>...</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>11.664000</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.411833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.166667</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.946667</td>\n",
       "      <td>1.363333</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.012633</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>42.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.333333</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>242.5</td>\n",
       "      <td>...</td>\n",
       "      <td>31.933333</td>\n",
       "      <td>11.80</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.886667</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.129067</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>81.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>190.666667</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>34.666667</td>\n",
       "      <td>303.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>10.00</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>3.976667</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   Age        CA      CREA    ALP    GGT         GLU         AST  \\\n",
       "0     1.0  82.0  2.090000  1.150000   95.0   40.0   78.000000   26.000000   \n",
       "3     0.0  82.0  2.270000  0.755000  123.5  176.5  106.000000  114.000000   \n",
       "4     1.0  79.0  2.070000  1.810000   62.0   36.5   96.000000   28.000000   \n",
       "5     1.0  84.0  2.060000  1.283333   75.0   75.0   95.500000   40.500000   \n",
       "8     0.0  48.0  2.110000  0.660000  200.0   90.0  104.000000   38.000000   \n",
       "...   ...   ...       ...       ...    ...    ...         ...         ...   \n",
       "1644  1.0  68.0  1.961070  0.688000   58.0   21.0   77.800000   12.200000   \n",
       "1650  0.0  79.0  1.923645  0.509000   44.0  206.6   93.000000   46.300000   \n",
       "1652  0.0  76.0  2.411833  0.750000   51.0   32.0   90.000000   25.333333   \n",
       "1678  1.0  58.0  2.012633  0.736667   42.0   20.0  100.333333   21.000000   \n",
       "1681  0.0  78.0  2.129067  0.483333   81.0   33.0  190.666667   21.000000   \n",
       "\n",
       "            ALT    LDH  ...         LY     MO        EO        BA        NET  \\\n",
       "0     21.000000  307.0  ...  13.400000   9.50  2.900000  0.500000   6.400000   \n",
       "3     63.000000  281.0  ...  36.500000   9.50  1.700000  0.900000   3.600000   \n",
       "4     38.500000  264.0  ...  44.000000  10.00  8.500000  0.500000   0.400000   \n",
       "5     27.000000  364.5  ...  17.000000   6.80  1.100000  0.600000   2.800000   \n",
       "8     36.000000  189.0  ...  12.500000   9.00  0.500000  0.000000   4.400000   \n",
       "...         ...    ...  ...        ...    ...       ...       ...        ...   \n",
       "1644  16.400000  185.0  ...  19.240000   7.44  1.320000  0.380000   3.986000   \n",
       "1650  33.700000  422.3  ...   7.110000   4.30  2.100000  0.170000  11.664000   \n",
       "1652  58.000000  233.0  ...  28.166667   8.00  2.200000  0.766667   2.946667   \n",
       "1678  35.000000  242.5  ...  31.933333  11.80  1.366667  0.100000   1.886667   \n",
       "1681  34.666667  303.0  ...  24.200000  10.00  3.866667  0.733333   3.976667   \n",
       "\n",
       "           LYT       MOT       EOT       BAT  Suspect  \n",
       "0     1.200000  0.800000  0.300000  0.000000      1.0  \n",
       "3     2.600000  0.700000  0.100000  0.100000      0.5  \n",
       "4     0.500000  0.100000  0.100000  0.000000      1.0  \n",
       "5     0.600000  0.250000  0.050000  0.000000      0.0  \n",
       "8     0.700000  0.500000  0.000000  0.000000      1.0  \n",
       "...        ...       ...       ...       ...      ...  \n",
       "1644  1.050000  0.404000  0.072000  0.022000      1.0  \n",
       "1650  0.895000  0.577000  0.286000  0.025000      1.0  \n",
       "1652  1.363333  0.386667  0.106667  0.036667      1.0  \n",
       "1678  1.116667  0.410000  0.050000  0.003333      1.0  \n",
       "1681  1.560000  0.646667  0.246667  0.046667      1.0  \n",
       "\n",
       "[826 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.iloc[:,:-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915c55c",
   "metadata": {},
   "source": [
    "We are splitting overall data except target column that is \"target\" in x variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed451e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "3       0\n",
       "4       0\n",
       "5       1\n",
       "8       0\n",
       "       ..\n",
       "1644    1\n",
       "1650    1\n",
       "1652    1\n",
       "1678    1\n",
       "1681    1\n",
       "Name: target, Length: 826, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y=df[\"target\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cb3ad",
   "metadata": {},
   "source": [
    "In that separate \"target\" column as a target in y variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4a059",
   "metadata": {},
   "source": [
    "#### Correlation of Feature and Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77a6b545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.corrwith(y).plot.bar(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a79be0",
   "metadata": {},
   "source": [
    "This bar shows the correlation between features(x) and target(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01187f55",
   "metadata": {},
   "source": [
    "### Balance the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5da3aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    463\n",
       "0    363\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72d6c1",
   "metadata": {},
   "source": [
    "Our target column having inbalance data.i.e 1=463 and 0=363\n",
    "We have to Balance the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df263412",
   "metadata": {},
   "source": [
    "#### Oversampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d676dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a37fd",
   "metadata": {},
   "source": [
    "For balancing data we use Oversampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c1fb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627c89d",
   "metadata": {},
   "source": [
    "We have to import Class RandomOverSampler and counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa1b8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros=RandomOverSampler(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23dbcad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ros,y_ros=ros.fit_resample(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a8e0e",
   "metadata": {},
   "source": [
    "Make the object of class and pass the data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc166773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Counter({1: 463, 0: 363})\n",
      "Resampled Dataset Counter({0: 463, 1: 463})\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\",Counter(y))\n",
    "print(\"Resampled Dataset\",Counter(y_ros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e54fc",
   "metadata": {},
   "source": [
    "Here is the output , first we print original data and then we have to see our data is in balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66167ef1",
   "metadata": {},
   "source": [
    "#### Apply Standard Scaler to Scale the data at one level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "154d2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e72aac",
   "metadata": {},
   "source": [
    "By using standard scaler we have data in one level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2fa199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b1fc4",
   "metadata": {},
   "source": [
    "For standard scaler we need to split data into training and testing parts, in that testing data is 20% and random state is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14287789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "xtrain=sc.fit_transform(xtrain)\n",
    "xtest=sc.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc717324",
   "metadata": {},
   "source": [
    "We make object of standard scaler. \n",
    "In that we have to pass xtrain and xtest for transform into standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c8daa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87177979,  1.1595822 , -0.51843211, ..., -0.2522526 ,\n",
       "        -0.45159523, -1.07639066],\n",
       "       [ 0.87177979,  1.1595822 ,  0.25074627, ..., -0.46243921,\n",
       "        -0.45159523, -2.67283524],\n",
       "       [-1.14707867,  1.27326673, -0.40447976, ..., -0.14715929,\n",
       "        -0.45159523,  0.52005392],\n",
       "       ...,\n",
       "       [ 0.87177979,  0.42063276,  1.30480553, ...,  0.16812063,\n",
       "        -0.45159523, -1.07639066],\n",
       "       [-1.14707867,  0.87537088, -0.37599167, ...,  0.79868046,\n",
       "        -0.45159523,  0.52005392],\n",
       "       [ 0.87177979,  1.27326673, -0.46145594, ..., -0.46243921,\n",
       "        -0.45159523, -1.07639066]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3543685",
   "metadata": {},
   "source": [
    "This is our standardize xtrain data on which we performed Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e66f36a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10151411, -0.72782876, -0.96412838, ..., -0.38556002,\n",
       "        -0.43393576,  0.49712499],\n",
       "       [ 0.9078413 ,  0.57897096, -1.02499824, ..., -0.38556002,\n",
       "        -0.43393576,  0.49712499],\n",
       "       [ 0.9078413 ,  0.57897096, -1.2076078 , ..., -0.38556002,\n",
       "        -0.43393576,  0.49712499],\n",
       "       ...,\n",
       "       [-1.10151411, -1.08422868,  0.61848784, ...,  0.25730895,\n",
       "         2.21922581,  0.49712499],\n",
       "       [ 0.9078413 ,  0.69777094, -1.2076078 , ..., -0.38556002,\n",
       "         0.89264502,  0.49712499],\n",
       "       [-1.10151411,  1.46997077, -1.32934751, ..., -0.38556002,\n",
       "        -0.43393576,  0.49712499]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac869b58",
   "metadata": {},
   "source": [
    "This is our standardize xtest data on which we performed Standard Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b7a7b",
   "metadata": {},
   "source": [
    "#### Train_Test_Split for separating data into training and testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfd9d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46052330",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4920f8",
   "metadata": {},
   "source": [
    "To build a module we have to saperate x and y for training and testing. In that test size is 20% and random state is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb019e6",
   "metadata": {},
   "source": [
    "#### Build a Model by using LogisticRegression Algoritham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bc81eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0df0d",
   "metadata": {},
   "source": [
    "From class sklearn.linear_model build a model Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517cf03",
   "metadata": {},
   "source": [
    "#### Classification Report of model trained on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2b16760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5dd1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression()\n",
    "lr.fit(xtrain,ytrain)\n",
    "ypred_train=lr.predict(xtrain)\n",
    "ypred_test=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14602bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       294\n",
      "           1       0.82      0.81      0.81       366\n",
      "\n",
      "    accuracy                           0.79       660\n",
      "   macro avg       0.79      0.79      0.79       660\n",
      "weighted avg       0.79      0.79      0.79       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        69\n",
      "           1       0.86      0.86      0.86        97\n",
      "\n",
      "    accuracy                           0.83       166\n",
      "   macro avg       0.83      0.83      0.83       166\n",
      "weighted avg       0.83      0.83      0.83       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68f8e6",
   "metadata": {},
   "source": [
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbf10a",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 79% and testing data is 83%. This is best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fb067",
   "metadata": {},
   "source": [
    "#### By using Hyperparameter or Hypertunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e023bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(solver=\"liblinear\")\n",
    "lr.fit(xtrain,ytrain)\n",
    "ypred_train=lr.predict(xtrain)\n",
    "ypred_test=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6b85e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       294\n",
      "           1       0.86      0.86      0.86       366\n",
      "\n",
      "    accuracy                           0.84       660\n",
      "   macro avg       0.84      0.84      0.84       660\n",
      "weighted avg       0.84      0.84      0.84       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        69\n",
      "           1       0.82      0.86      0.84        97\n",
      "\n",
      "    accuracy                           0.81       166\n",
      "   macro avg       0.80      0.80      0.80       166\n",
      "weighted avg       0.81      0.81      0.81       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd1345b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(solver=\"sag\")\n",
    "lr.fit(xtrain,ytrain)\n",
    "ypred_train=lr.predict(xtrain)\n",
    "ypred_test=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f01c7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       294\n",
      "           1       0.80      0.77      0.78       366\n",
      "\n",
      "    accuracy                           0.76       660\n",
      "   macro avg       0.76      0.76      0.76       660\n",
      "weighted avg       0.76      0.76      0.76       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76        69\n",
      "           1       0.82      0.85      0.83        97\n",
      "\n",
      "    accuracy                           0.80       166\n",
      "   macro avg       0.80      0.79      0.79       166\n",
      "weighted avg       0.80      0.80      0.80       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39c3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(solver=\"saga\")\n",
    "lr.fit(xtrain,ytrain)\n",
    "ypred_train=lr.predict(xtrain)\n",
    "ypred_test=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53a9af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       294\n",
      "           1       0.78      0.75      0.77       366\n",
      "\n",
      "    accuracy                           0.75       660\n",
      "   macro avg       0.74      0.75      0.74       660\n",
      "weighted avg       0.75      0.75      0.75       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76        69\n",
      "           1       0.82      0.85      0.83        97\n",
      "\n",
      "    accuracy                           0.80       166\n",
      "   macro avg       0.80      0.79      0.79       166\n",
      "weighted avg       0.80      0.80      0.80       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34e3e32",
   "metadata": {},
   "source": [
    "By using Hypertunning Parameters i.e \"liblenear\", \"sag\",\"saga\" we have best fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f40af0",
   "metadata": {},
   "source": [
    "#### Build a model by using KNN Algoritham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51d33a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d84c40",
   "metadata": {},
   "source": [
    "From class sklearn.neighbors build a model KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60abbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(xtrain,ytrain)\n",
    "ypred_train=knn.predict(xtrain)\n",
    "ypred_test=knn.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27d701",
   "metadata": {},
   "source": [
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da2df3",
   "metadata": {},
   "source": [
    "#### Evaluate the model by using Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ed77f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b58be17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79       294\n",
      "           1       0.82      0.86      0.84       366\n",
      "\n",
      "    accuracy                           0.82       660\n",
      "   macro avg       0.82      0.81      0.81       660\n",
      "weighted avg       0.82      0.82      0.82       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.57      0.66        69\n",
      "           1       0.74      0.90      0.81        97\n",
      "\n",
      "    accuracy                           0.76       166\n",
      "   macro avg       0.77      0.73      0.74       166\n",
      "weighted avg       0.77      0.76      0.75       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bbbfa",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 82% and testing data is 76%. This is best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457d72a",
   "metadata": {},
   "source": [
    "#### Build a model on SVM Algoritham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e593f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c671e4f",
   "metadata": {},
   "source": [
    "From class sklearn.svm build a model svc(suport vector machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d07070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daaa97f",
   "metadata": {},
   "source": [
    "Create A object of class svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7b86cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(model):\n",
    "    model.fit(xtrain,ytrain)\n",
    "    ypred=model.predict(xtest)\n",
    "    print(classification_report(ytest,ypred))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16576d",
   "metadata": {},
   "source": [
    "We are making one function i.e mymodel in that we have to pass our model name.\n",
    "\n",
    "Fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "Predicted values are stored in a ypred variable.\n",
    "\n",
    "we have to print classification report of this model for predict accuracy of trainning data.\n",
    "\n",
    "In return we have to pass name of our module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32404e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c962aac",
   "metadata": {},
   "source": [
    "From class sklearn.pipline we have to import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518bebf",
   "metadata": {},
   "source": [
    "We using pipeline beacause data should be process smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66b5e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe=Pipeline(steps=[('scaler',StandardScaler()),('svm',SVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4101b8",
   "metadata": {},
   "source": [
    "In pipe we write steps so that steps will be executed one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59631638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(xtrain,ytrain)\n",
    "ypred=pipe.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2be2dbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.75        69\n",
      "           1       0.81      0.85      0.83        97\n",
      "\n",
      "    accuracy                           0.80       166\n",
      "   macro avg       0.79      0.78      0.79       166\n",
      "weighted avg       0.79      0.80      0.79       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81683b14",
   "metadata": {},
   "source": [
    "fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred is variable that stored predicted values by model of testing data\n",
    "\n",
    "We have to call the function and pass model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7c16a",
   "metadata": {},
   "source": [
    "#### By using Hyperparameter or Hypertunners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50e9ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78        69\n",
      "           1       0.84      0.84      0.84        97\n",
      "\n",
      "    accuracy                           0.81       166\n",
      "   macro avg       0.81      0.81      0.81       166\n",
      "weighted avg       0.81      0.81      0.81       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm=SVC(kernel='linear')\n",
    "mymodel(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ee2bf1",
   "metadata": {},
   "source": [
    "By using linear kernel i.e hypertunning parameter we are having accuracy 81%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2933004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.54      0.49        69\n",
      "           1       0.61      0.53      0.57        97\n",
      "\n",
      "    accuracy                           0.53       166\n",
      "   macro avg       0.53      0.53      0.53       166\n",
      "weighted avg       0.54      0.53      0.53       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;sigmoid&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;sigmoid&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='sigmoid')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm=SVC(kernel='sigmoid')\n",
    "mymodel(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82c913",
   "metadata": {},
   "source": [
    "By using sigmoid kernel i.e hypertunning parameter we are having accuracy 53%. Its very low accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2649cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.74        69\n",
      "           1       0.84      0.72      0.78        97\n",
      "\n",
      "    accuracy                           0.76       166\n",
      "   macro avg       0.76      0.77      0.76       166\n",
      "weighted avg       0.77      0.76      0.76       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm=SVC(kernel='poly')\n",
    "mymodel(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937c4ed",
   "metadata": {},
   "source": [
    "By using poly kernel i.e hypertunning parameter we are having accuracy 76%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aec25a",
   "metadata": {},
   "source": [
    "#### Build a model by using Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c846411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc7d1b",
   "metadata": {},
   "source": [
    "From class sklearn.tree build a model Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfe0b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(xtrain,ytrain)\n",
    "ytrain_pred=dt.predict(xtrain)\n",
    "ytest_pred=dt.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c59a5",
   "metadata": {},
   "source": [
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420e5e3",
   "metadata": {},
   "source": [
    "#### Evaluate the model by using Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90cca06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7db54a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       294\n",
      "           1       1.00      1.00      1.00       366\n",
      "\n",
      "    accuracy                           1.00       660\n",
      "   macro avg       1.00      1.00      1.00       660\n",
      "weighted avg       1.00      1.00      1.00       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73        69\n",
      "           1       0.81      0.79      0.80        97\n",
      "\n",
      "    accuracy                           0.77       166\n",
      "   macro avg       0.76      0.77      0.77       166\n",
      "weighted avg       0.77      0.77      0.77       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ytrain_pred))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ytest_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee047f08",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data should be 100% its high bias and testing data is 76% its variance is high. Beacause of this scenario it is underfitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb2d5e",
   "metadata": {},
   "source": [
    "#### Performing Hypertunning on model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dec010",
   "metadata": {},
   "source": [
    "#### Hyper parameters with impurity checking gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89e1286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth:1 accuracy): 0.7289156626506024\n",
      "Max Depth:2 accuracy): 0.7650602409638554\n",
      "Max Depth:3 accuracy): 0.7951807228915663\n",
      "Max Depth:4 accuracy): 0.7530120481927711\n",
      "Max Depth:5 accuracy): 0.7771084337349398\n",
      "Max Depth:6 accuracy): 0.7771084337349398\n",
      "Max Depth:7 accuracy): 0.7891566265060241\n",
      "Max Depth:8 accuracy): 0.8012048192771084\n",
      "Max Depth:9 accuracy): 0.7530120481927711\n",
      "Max Depth:10 accuracy): 0.7590361445783133\n",
      "Max Depth:11 accuracy): 0.7710843373493976\n",
      "Max Depth:12 accuracy): 0.7650602409638554\n",
      "Max Depth:13 accuracy): 0.7349397590361446\n",
      "Max Depth:14 accuracy): 0.7710843373493976\n",
      "Max Depth:15 accuracy): 0.7289156626506024\n",
      "Max Depth:16 accuracy): 0.7771084337349398\n",
      "Max Depth:17 accuracy): 0.7590361445783133\n",
      "Max Depth:18 accuracy): 0.7469879518072289\n",
      "Max Depth:19 accuracy): 0.7530120481927711\n",
      "Max Depth:20 accuracy): 0.7710843373493976\n",
      "Max Depth:21 accuracy): 0.7530120481927711\n",
      "Max Depth:22 accuracy): 0.7530120481927711\n",
      "Max Depth:23 accuracy): 0.7590361445783133\n",
      "Max Depth:24 accuracy): 0.7469879518072289\n",
      "Max Depth:25 accuracy): 0.7831325301204819\n",
      "Max Depth:26 accuracy): 0.7530120481927711\n",
      "Max Depth:27 accuracy): 0.7530120481927711\n",
      "Max Depth:28 accuracy): 0.7409638554216867\n",
      "Max Depth:29 accuracy): 0.7349397590361446\n",
      "Max Depth:30 accuracy): 0.7590361445783133\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,31):\n",
    "    dt1=DecisionTreeClassifier(max_depth=i)\n",
    "    dt1.fit(xtrain,ytrain)\n",
    "    ypred=dt1.predict(xtest)\n",
    "    ac=accuracy_score(ytest,ypred)\n",
    "    print(f\"Max Depth:{i} accuracy): {ac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c05dce",
   "metadata": {},
   "source": [
    "We splin a loop in range 1 to 31, Dt model apply on this range and this values are stored into dt1 variable.\n",
    "\n",
    "Fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "Ypred is a variable its stored predicted values of xtest\n",
    "\n",
    "Accuracy score of this values are stored in ac variable.\n",
    "\n",
    "print maximum depth and accuracy score of each ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6df34",
   "metadata": {},
   "source": [
    "#### Building model with final value of max depth as 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fec1963e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77        69\n",
      "           1       0.85      0.78      0.82        97\n",
      "\n",
      "    accuracy                           0.80       166\n",
      "   macro avg       0.79      0.80      0.79       166\n",
      "weighted avg       0.80      0.80      0.80       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1=DecisionTreeClassifier(max_depth=3)\n",
    "mymodel(dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc970b41",
   "metadata": {},
   "source": [
    "In these accuracy we want to select the max depth in wich accuracy score is decreases. In these scenario max depth 3 is the point when accuracy score was decrease.\n",
    "\n",
    "With the help of gini index Hypertunning parameter accuracy is be 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02795ae",
   "metadata": {},
   "source": [
    "#### Lets checking overfitting scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38acda23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909090909090909"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1.score(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6e6ff",
   "metadata": {},
   "source": [
    "With the help of gini index Hypertunning parameter model is best fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55163e",
   "metadata": {},
   "source": [
    "#### Hyper parameters with impurity checking min_sample_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "329a5577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min sample split:5 accuracy): 0.7891566265060241\n",
      "min sample split:6 accuracy): 0.7831325301204819\n",
      "min sample split:7 accuracy): 0.7710843373493976\n",
      "min sample split:8 accuracy): 0.7891566265060241\n",
      "min sample split:9 accuracy): 0.7951807228915663\n",
      "min sample split:10 accuracy): 0.7650602409638554\n",
      "min sample split:11 accuracy): 0.7590361445783133\n",
      "min sample split:12 accuracy): 0.7650602409638554\n",
      "min sample split:13 accuracy): 0.7650602409638554\n",
      "min sample split:14 accuracy): 0.7530120481927711\n",
      "min sample split:15 accuracy): 0.7590361445783133\n",
      "min sample split:16 accuracy): 0.7590361445783133\n",
      "min sample split:17 accuracy): 0.7710843373493976\n",
      "min sample split:18 accuracy): 0.7771084337349398\n",
      "min sample split:19 accuracy): 0.7650602409638554\n",
      "min sample split:20 accuracy): 0.7590361445783133\n",
      "min sample split:21 accuracy): 0.7590361445783133\n",
      "min sample split:22 accuracy): 0.7710843373493976\n",
      "min sample split:23 accuracy): 0.7710843373493976\n",
      "min sample split:24 accuracy): 0.7710843373493976\n",
      "min sample split:25 accuracy): 0.7710843373493976\n",
      "min sample split:26 accuracy): 0.7650602409638554\n",
      "min sample split:27 accuracy): 0.7590361445783133\n",
      "min sample split:28 accuracy): 0.7590361445783133\n",
      "min sample split:29 accuracy): 0.7771084337349398\n",
      "min sample split:30 accuracy): 0.7710843373493976\n",
      "min sample split:31 accuracy): 0.7710843373493976\n",
      "min sample split:32 accuracy): 0.7771084337349398\n",
      "min sample split:33 accuracy): 0.7710843373493976\n",
      "min sample split:34 accuracy): 0.7710843373493976\n",
      "min sample split:35 accuracy): 0.7771084337349398\n",
      "min sample split:36 accuracy): 0.7710843373493976\n",
      "min sample split:37 accuracy): 0.7771084337349398\n",
      "min sample split:38 accuracy): 0.7710843373493976\n",
      "min sample split:39 accuracy): 0.7891566265060241\n",
      "min sample split:40 accuracy): 0.7831325301204819\n",
      "min sample split:41 accuracy): 0.7891566265060241\n",
      "min sample split:42 accuracy): 0.7891566265060241\n",
      "min sample split:43 accuracy): 0.7831325301204819\n",
      "min sample split:44 accuracy): 0.7891566265060241\n",
      "min sample split:45 accuracy): 0.7891566265060241\n",
      "min sample split:46 accuracy): 0.7891566265060241\n",
      "min sample split:47 accuracy): 0.7891566265060241\n",
      "min sample split:48 accuracy): 0.7710843373493976\n",
      "min sample split:49 accuracy): 0.7710843373493976\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,50):\n",
    "    dt3=DecisionTreeClassifier(min_samples_split=i)\n",
    "    dt3.fit(xtrain,ytrain)\n",
    "    ypred=dt3.predict(xtest)\n",
    "    ac=accuracy_score(ytest,ypred)\n",
    "    print(f\"min sample split:{i} accuracy): {ac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f90f9",
   "metadata": {},
   "source": [
    "We splin a loop in range 5 to 50, Dt model apply on this range and this values are stored into dt3 variable.\n",
    "\n",
    "Fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "Ypred is a variable its stored predicted values of xtest\n",
    "\n",
    "Accuracy score of this values are stored in ac variable.\n",
    "\n",
    "print values if min_samples_split and accuracy score of each ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40333cab",
   "metadata": {},
   "source": [
    "#### Building model with final value of max depth as 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b49802c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72        69\n",
      "           1       0.81      0.78      0.80        97\n",
      "\n",
      "    accuracy                           0.77       166\n",
      "   macro avg       0.76      0.76      0.76       166\n",
      "weighted avg       0.77      0.77      0.77       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(min_samples_split=8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(min_samples_split=8)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(min_samples_split=8)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt3=DecisionTreeClassifier(min_samples_split=8)\n",
    "mymodel(dt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcb569",
   "metadata": {},
   "source": [
    "In these accuracy we want to select the values ofmin_samples_split in wich accuracy score is decreases. In these scenario min_samples_split=8 is the point when accuracy score was decrease.\n",
    "\n",
    "With the help of min_samples_split Hypertunning parameter accuracy is be 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58953cd8",
   "metadata": {},
   "source": [
    "#### Lets checking overfitting scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4dd30995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636363636363636"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt3.score(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6f8ac",
   "metadata": {},
   "source": [
    "With the help of min_samples_split Hypertunning parameter model is over fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259a942",
   "metadata": {},
   "source": [
    "#### Hyper parameters with impurity checking min_sample_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33f05a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min sample leaf:1 accuracy): 0.7650602409638554\n",
      "min sample leaf:2 accuracy): 0.7650602409638554\n",
      "min sample leaf:3 accuracy): 0.7771084337349398\n",
      "min sample leaf:4 accuracy): 0.7590361445783133\n",
      "min sample leaf:5 accuracy): 0.7108433734939759\n",
      "min sample leaf:6 accuracy): 0.7530120481927711\n",
      "min sample leaf:7 accuracy): 0.7228915662650602\n",
      "min sample leaf:8 accuracy): 0.7228915662650602\n",
      "min sample leaf:9 accuracy): 0.7710843373493976\n",
      "min sample leaf:10 accuracy): 0.7469879518072289\n",
      "min sample leaf:11 accuracy): 0.7710843373493976\n",
      "min sample leaf:12 accuracy): 0.7590361445783133\n",
      "min sample leaf:13 accuracy): 0.7710843373493976\n",
      "min sample leaf:14 accuracy): 0.7710843373493976\n",
      "min sample leaf:15 accuracy): 0.7831325301204819\n",
      "min sample leaf:16 accuracy): 0.7831325301204819\n",
      "min sample leaf:17 accuracy): 0.7710843373493976\n",
      "min sample leaf:18 accuracy): 0.7228915662650602\n",
      "min sample leaf:19 accuracy): 0.7710843373493976\n",
      "min sample leaf:20 accuracy): 0.8012048192771084\n",
      "min sample leaf:21 accuracy): 0.7951807228915663\n",
      "min sample leaf:22 accuracy): 0.7951807228915663\n",
      "min sample leaf:23 accuracy): 0.7951807228915663\n",
      "min sample leaf:24 accuracy): 0.7951807228915663\n",
      "min sample leaf:25 accuracy): 0.7951807228915663\n",
      "min sample leaf:26 accuracy): 0.7951807228915663\n",
      "min sample leaf:27 accuracy): 0.7951807228915663\n",
      "min sample leaf:28 accuracy): 0.7951807228915663\n",
      "min sample leaf:29 accuracy): 0.7951807228915663\n",
      "min sample leaf:30 accuracy): 0.7951807228915663\n",
      "min sample leaf:31 accuracy): 0.8072289156626506\n",
      "min sample leaf:32 accuracy): 0.8072289156626506\n",
      "min sample leaf:33 accuracy): 0.7951807228915663\n",
      "min sample leaf:34 accuracy): 0.7951807228915663\n",
      "min sample leaf:35 accuracy): 0.7951807228915663\n",
      "min sample leaf:36 accuracy): 0.7951807228915663\n",
      "min sample leaf:37 accuracy): 0.7951807228915663\n",
      "min sample leaf:38 accuracy): 0.7951807228915663\n",
      "min sample leaf:39 accuracy): 0.7951807228915663\n",
      "min sample leaf:40 accuracy): 0.7951807228915663\n",
      "min sample leaf:41 accuracy): 0.7951807228915663\n",
      "min sample leaf:42 accuracy): 0.7951807228915663\n",
      "min sample leaf:43 accuracy): 0.7951807228915663\n",
      "min sample leaf:44 accuracy): 0.7951807228915663\n",
      "min sample leaf:45 accuracy): 0.7951807228915663\n",
      "min sample leaf:46 accuracy): 0.7951807228915663\n",
      "min sample leaf:47 accuracy): 0.7951807228915663\n",
      "min sample leaf:48 accuracy): 0.7951807228915663\n",
      "min sample leaf:49 accuracy): 0.7951807228915663\n",
      "min sample leaf:50 accuracy): 0.7951807228915663\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,51):\n",
    "    dt4=DecisionTreeClassifier(min_samples_leaf=i)\n",
    "    dt4.fit(xtrain,ytrain)\n",
    "    ypred=dt4.predict(xtest)\n",
    "    ac=accuracy_score(ytest,ypred)\n",
    "    print(f\"min sample leaf:{i} accuracy): {ac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e37a06",
   "metadata": {},
   "source": [
    "We splin a loop in range 1 to 51, Dt model apply on this range and this values are stored into dt4 variable.\n",
    "\n",
    "Fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "Ypred is a variable its stored predicted values of xtest\n",
    "\n",
    "Accuracy score of this values are stored in ac variable.\n",
    "\n",
    "print values if min_samples_leaf and accuracy score of each ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e53a0f",
   "metadata": {},
   "source": [
    "#### Building model with final value of max depth as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4313b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.80      0.73        69\n",
      "           1       0.84      0.73      0.78        97\n",
      "\n",
      "    accuracy                           0.76       166\n",
      "   macro avg       0.76      0.76      0.76       166\n",
      "weighted avg       0.77      0.76      0.76       166\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(min_samples_leaf=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(min_samples_leaf=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(min_samples_leaf=2)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt4=DecisionTreeClassifier(min_samples_leaf=2)\n",
    "mymodel(dt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70fde7",
   "metadata": {},
   "source": [
    "In these accuracy we want to select the values of min_samples_leaf in wich accuracy score is decreases. In these scenario min_samples_leaf=8 is the point when accuracy score was decrease.\n",
    "\n",
    "With the help of min_samples_leaf Hypertunning parameter accuracy is be 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272c84b",
   "metadata": {},
   "source": [
    "#### Lets checking overfitting scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cee3df06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606060606060606"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt4.score(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cf26c6",
   "metadata": {},
   "source": [
    "With the help of min_samples_leaf Hypertunning parameter model is over fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c30129",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35233914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938f884",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "915f0be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg=BaggingClassifier(LogisticRegression())\n",
    "bg.fit(xtrain,ytrain)\n",
    "ypred_train=bg.predict(xtrain)\n",
    "ypred_test=bg.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a18e8c",
   "metadata": {},
   "source": [
    "Craete a object of this class and pass a module (Logistic regression) in that and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d986148",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd3c1e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77       294\n",
      "           1       0.81      0.83      0.82       366\n",
      "\n",
      "    accuracy                           0.80       660\n",
      "   macro avg       0.79      0.79      0.79       660\n",
      "weighted avg       0.80      0.80      0.80       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78        69\n",
      "           1       0.85      0.85      0.85        97\n",
      "\n",
      "    accuracy                           0.82       166\n",
      "   macro avg       0.81      0.81      0.81       166\n",
      "weighted avg       0.82      0.82      0.82       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bc638",
   "metadata": {},
   "source": [
    "According to classification Report accuracy through Logistic Regression of training data is 80% and testing data is 83%. This is best fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ef967c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg=BaggingClassifier(DecisionTreeClassifier())\n",
    "bg.fit(xtrain,ytrain)\n",
    "ypred_train=bg.predict(xtrain)\n",
    "ypred_test=bg.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c060b",
   "metadata": {},
   "source": [
    "Craete a object of this class and pass a module (DecisionTreeClassifier) in that and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66038eb",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ceaad04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       294\n",
      "           1       1.00      0.99      0.99       366\n",
      "\n",
      "    accuracy                           0.99       660\n",
      "   macro avg       0.99      0.99      0.99       660\n",
      "weighted avg       0.99      0.99      0.99       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.78        69\n",
      "           1       0.86      0.81      0.84        97\n",
      "\n",
      "    accuracy                           0.81       166\n",
      "   macro avg       0.81      0.81      0.81       166\n",
      "weighted avg       0.82      0.81      0.81       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd9e0fd",
   "metadata": {},
   "source": [
    "According to classification Report accuracy through DecisionTreeClassifier of training data is 99% and testing data is 82%. This is over fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1dd95327",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg=BaggingClassifier(KNeighborsClassifier())\n",
    "bg.fit(xtrain,ytrain)\n",
    "ypred_train=bg.predict(xtrain)\n",
    "ypred_test=bg.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa3f0c",
   "metadata": {},
   "source": [
    "Craete a object of this class and pass a module (KNeighborsClassifier) in that and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c08b7",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a92c6e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77       294\n",
      "           1       0.81      0.84      0.82       366\n",
      "\n",
      "    accuracy                           0.80       660\n",
      "   macro avg       0.80      0.80      0.80       660\n",
      "weighted avg       0.80      0.80      0.80       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.49      0.61        69\n",
      "           1       0.72      0.91      0.80        97\n",
      "\n",
      "    accuracy                           0.73       166\n",
      "   macro avg       0.75      0.70      0.70       166\n",
      "weighted avg       0.75      0.73      0.72       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25838f54",
   "metadata": {},
   "source": [
    "According to classification Report accuracy through KNeighborsClassifier of training data is 82% and testing data is 78%. This is best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93454e",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "60e9f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf=RandomForestClassifier()\n",
    "rf.fit(xtrain,ytrain)\n",
    "ypred_train=rf.predict(xtrain)\n",
    "ypred_test=rf.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f53e4",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model RandomForestClassifier\n",
    "\n",
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8019c9",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2289de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       294\n",
      "           1       1.00      1.00      1.00       366\n",
      "\n",
      "    accuracy                           1.00       660\n",
      "   macro avg       1.00      1.00      1.00       660\n",
      "weighted avg       1.00      1.00      1.00       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81        69\n",
      "           1       0.85      0.89      0.87        97\n",
      "\n",
      "    accuracy                           0.84       166\n",
      "   macro avg       0.84      0.83      0.84       166\n",
      "weighted avg       0.84      0.84      0.84       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b396e",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data should be 100% its high bias and testing data is 86% its variance is high. Beacause of this scenario it is underfitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ebe94",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e47aa259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08508414",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c0eb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=[]\n",
    "model.append((\"Logistic Regression\",LogisticRegression()))\n",
    "model.append((\"Decision Tree\",DecisionTreeClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a9f5287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression', LogisticRegression()),\n",
       " ('Decision Tree', DecisionTreeClassifier())]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b472e",
   "metadata": {},
   "source": [
    "Make a two lists i.e. Model, Accuracy. \n",
    "Logistic and Decision Tree algo uppend in the model list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "995b0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc=VotingClassifier(estimators=model)\n",
    "vc.fit(xtrain,ytrain)\n",
    "ypred_train=vc.predict(xtrain)\n",
    "ypred_test=vc.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1459780",
   "metadata": {},
   "source": [
    "Craete a object of this class and pass estimators is model and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78526ed9",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89b3e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       294\n",
      "           1       1.00      0.81      0.89       366\n",
      "\n",
      "    accuracy                           0.89       660\n",
      "   macro avg       0.90      0.90      0.89       660\n",
      "weighted avg       0.91      0.89      0.89       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77        69\n",
      "           1       0.90      0.68      0.78        97\n",
      "\n",
      "    accuracy                           0.77       166\n",
      "   macro avg       0.79      0.79      0.77       166\n",
      "weighted avg       0.81      0.77      0.77       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21500e51",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 89%  and testing data is 81% . Beacause of this scenario it is best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338ec82",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee840570",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "173cfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ad=AdaBoostClassifier()\n",
    "ad.fit(xtrain,ytrain)\n",
    "ypred_train=ad.predict(xtrain)\n",
    "ypred_test=ad.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdba2d",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model AdaBoostClassifier\n",
    "\n",
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0f3de",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55a83821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       294\n",
      "           1       0.89      0.90      0.89       366\n",
      "\n",
      "    accuracy                           0.88       660\n",
      "   macro avg       0.88      0.88      0.88       660\n",
      "weighted avg       0.88      0.88      0.88       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77        69\n",
      "           1       0.84      0.84      0.84        97\n",
      "\n",
      "    accuracy                           0.81       166\n",
      "   macro avg       0.80      0.80      0.80       166\n",
      "weighted avg       0.81      0.81      0.81       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada8bcf",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 88%  and testing data is 81% . Beacause of this scenario it is best fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366c7d2",
   "metadata": {},
   "source": [
    "### GBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1b06a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gd=GradientBoostingClassifier()\n",
    "gd.fit(xtrain,ytrain)\n",
    "ypred_train=gd.predict(xtrain)\n",
    "ypred_test=gd.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c523e9c",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model GradientBoostingClassifier\n",
    "\n",
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d4bb8f",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ea92626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       294\n",
      "           1       0.97      0.99      0.98       366\n",
      "\n",
      "    accuracy                           0.98       660\n",
      "   macro avg       0.98      0.97      0.98       660\n",
      "weighted avg       0.98      0.98      0.98       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.78        69\n",
      "           1       0.83      0.87      0.85        97\n",
      "\n",
      "    accuracy                           0.82       166\n",
      "   macro avg       0.82      0.81      0.81       166\n",
      "weighted avg       0.82      0.82      0.82       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff8683",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 98%  and testing data is 82% . Beacause of this scenario it is over fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769472f3",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57fe549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xg=XGBClassifier()\n",
    "xg.fit(xtrain,ytrain)\n",
    "ypred_train=xg.predict(xtrain)\n",
    "ypred_test=xg.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db95a98",
   "metadata": {},
   "source": [
    "From class sklearn.ensemble build a model XGBClassifier\n",
    "\n",
    "Craete a object of this class and fit() method is used in order to train the model and use it for predictions.\n",
    "\n",
    "ypred_train is variable that stored predicted values by model of training data.\n",
    "\n",
    "ypred_test is variable that stored predicted values by model of testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464160ab",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8ad619fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       294\n",
      "           1       1.00      1.00      1.00       366\n",
      "\n",
      "    accuracy                           1.00       660\n",
      "   macro avg       1.00      1.00      1.00       660\n",
      "weighted avg       1.00      1.00      1.00       660\n",
      "\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        69\n",
      "           1       0.82      0.86      0.84        97\n",
      "\n",
      "    accuracy                           0.81       166\n",
      "   macro avg       0.80      0.80      0.80       166\n",
      "weighted avg       0.81      0.81      0.81       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Data\")\n",
    "print(classification_report(ytrain,ypred_train))\n",
    "print(\"Test Data\")\n",
    "print(classification_report(ytest,ypred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f52e1",
   "metadata": {},
   "source": [
    "According to classification Report accuracy of training data is 100% that is high bias and testing data is 81% with high varieance . Beacause of this scenario it is over fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2235e",
   "metadata": {},
   "source": [
    "# The Best Prediction was done by model trained on AdaBoost Algoritham with accuracy of 88% on Train Data and 81% on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f95443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
